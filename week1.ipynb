{
 "cells": [
  {
   "cell_type": "code",
   "id": "a730d195",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T12:48:51.398556Z",
     "start_time": "2025-11-28T12:48:43.582705Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "torch.cuda.is_available()\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "ea8536de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T12:48:51.719901Z",
     "start_time": "2025-11-28T12:48:51.685159Z"
    }
   },
   "source": [
    "# Combined dataset: bikes for short distances, cars for longer ones\n",
    "distances = torch.tensor([\n",
    "    [1.0], [1.5], [2.0], [2.5], [3.0], [3.5], [4.0], [4.5], [5.0], [5.5],\n",
    "    [6.0], [6.5], [7.0], [7.5], [8.0], [8.5], [9.0], [9.5], [10.0], [10.5],\n",
    "    [11.0], [11.5], [12.0], [12.5], [13.0], [13.5], [14.0], [14.5], [15.0], [15.5],\n",
    "    [16.0], [16.5], [17.0], [17.5], [18.0], [18.5], [19.0], [19.5], [20.0]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Corresponding delivery times in minutes\n",
    "times = torch.tensor([\n",
    "    [6.96], [9.67], [12.11], [14.56], [16.77], [21.7], [26.52], [32.47], [37.15], [42.35],\n",
    "    [46.1], [52.98], [57.76], [61.29], [66.15], [67.63], [69.45], [71.57], [72.8], [73.88],\n",
    "    [76.34], [76.38], [78.34], [80.07], [81.86], [84.45], [83.98], [86.55], [88.33], [86.83],\n",
    "    [89.24], [88.11], [88.16], [91.77], [92.27], [92.13], [90.73], [90.39], [92.98]\n",
    "], dtype=torch.float32)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "07bbf181",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T12:48:51.841597Z",
     "start_time": "2025-11-28T12:48:51.791659Z"
    }
   },
   "source": [
    "distances_std = distances.std()\n",
    "distances_mean = distances.mean()\n",
    "\n",
    "times_std = times.std()\n",
    "times_mean = times.mean()\n",
    "\n",
    "distances_norm = (distances-distances_mean)/distances_std\n",
    "times_norm = (times - times_mean)/times_std"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "053868bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T12:48:51.896266Z",
     "start_time": "2025-11-28T12:48:51.880916Z"
    }
   },
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(1,3),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(3,1)\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "56d9b653",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T12:48:56.339493Z",
     "start_time": "2025-11-28T12:48:51.943706Z"
    }
   },
   "source": [
    "loss_function = nn.MSELoss()\n",
    "gradient = optim.SGD(model.parameters(),lr=0.01)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69719665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 50 and Loss :0.5651528835296631\n",
      "Epoch : 100 and Loss :0.2961590588092804\n",
      "Epoch : 150 and Loss :0.14635375142097473\n",
      "Epoch : 200 and Loss :0.07368024438619614\n",
      "Epoch : 250 and Loss :0.04094141349196434\n",
      "Epoch : 300 and Loss :0.026829881593585014\n",
      "Epoch : 350 and Loss :0.020816726610064507\n",
      "Epoch : 400 and Loss :0.017968837171792984\n",
      "Epoch : 450 and Loss :0.016489338129758835\n",
      "Epoch : 500 and Loss :0.015624321065843105\n",
      "Epoch : 550 and Loss :0.015039408579468727\n",
      "Epoch : 600 and Loss :0.01459487620741129\n",
      "Epoch : 650 and Loss :0.01423056423664093\n",
      "Epoch : 700 and Loss :0.013918861746788025\n",
      "Epoch : 750 and Loss :0.013520519249141216\n",
      "Epoch : 800 and Loss :0.013058059848845005\n",
      "Epoch : 850 and Loss :0.01264489907771349\n",
      "Epoch : 900 and Loss :0.01227481383830309\n",
      "Epoch : 950 and Loss :0.01194242388010025\n",
      "Epoch : 1000 and Loss :0.011643080040812492\n",
      "\n",
      "Training Complete.\n",
      "\n",
      "Final Loss: 0.011643080040812492\n"
     ]
    }
   ],
   "source": [
    "for epoches in range(1000):\n",
    "    gradient.zero_grad()\n",
    "\n",
    "    output = model(distances_norm)\n",
    "\n",
    "    loss = loss_function(output, times_norm)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    gradient.step()\n",
    "\n",
    "    if (epoches + 1) % 50 == 0:\n",
    "        print(f\"Epoch : {epoches+1} and Loss :{loss.item()}\" )\n",
    "        \n",
    "\n",
    "print(\"\\nTraining Complete.\")\n",
    "print(f\"\\nFinal Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11cf5d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_d = 5.1\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f2128c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You need tensor([[37.4755]]) to reach 5.1\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    new_distance = torch.tensor([[new_d]], dtype=torch.float32)\n",
    "\n",
    "    new_dist_norm = (new_distance - distances_mean)/distances_std\n",
    "\n",
    "    output = model(new_dist_norm)\n",
    "\n",
    "    outputnorm = (output*times_std) + times_mean\n",
    "\n",
    "    print(f\"You need {outputnorm} to reach {new_d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2677b229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./datasets./data_with_features.csv\"\n",
    "data = pd.read_csv(path)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d26adcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "distance_miles",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "time_of_day_hours",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "is_weekend",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "delivery_time_minutes",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "db376e7a-6145-4942-b5ad-8c55fa2346fa",
       "rows": [
        [
         "0",
         "1.6",
         "8.2",
         "0",
         "7.22"
        ],
        [
         "1",
         "13.09",
         "16.8",
         "1",
         "32.41"
        ],
        [
         "2",
         "6.97",
         "8.02",
         "1",
         "17.47"
        ],
        [
         "3",
         "10.66",
         "16.07",
         "0",
         "37.17"
        ],
        [
         "4",
         "18.24",
         "13.47",
         "0",
         "38.36"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance_miles</th>\n",
       "      <th>time_of_day_hours</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>delivery_time_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.60</td>\n",
       "      <td>8.20</td>\n",
       "      <td>0</td>\n",
       "      <td>7.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.09</td>\n",
       "      <td>16.80</td>\n",
       "      <td>1</td>\n",
       "      <td>32.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.97</td>\n",
       "      <td>8.02</td>\n",
       "      <td>1</td>\n",
       "      <td>17.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.66</td>\n",
       "      <td>16.07</td>\n",
       "      <td>0</td>\n",
       "      <td>37.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.24</td>\n",
       "      <td>13.47</td>\n",
       "      <td>0</td>\n",
       "      <td>38.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   distance_miles  time_of_day_hours  is_weekend  delivery_time_minutes\n",
       "0            1.60               8.20           0                   7.22\n",
       "1           13.09              16.80           1                  32.41\n",
       "2            6.97               8.02           1                  17.47\n",
       "3           10.66              16.07           0                  37.17\n",
       "4           18.24              13.47           0                  38.36"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "id": "80ee1f83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T12:51:16.156390Z",
     "start_time": "2025-11-28T12:51:16.149973Z"
    }
   },
   "source": [
    "def rush_hour_features(hours, weekends):\n",
    "    is_morning_rush = (hours >= 8) & (hours <= 10)\n",
    "    is_evening_rush = (hours >= 16) & (hours <= 19)\n",
    "\n",
    "    is_weekday = weekends == 0\n",
    "\n",
    "    rush_hour_mask = (is_morning_rush | is_evening_rush) & is_weekday\n",
    "\n",
    "    return rush_hour_mask.unsqueeze(1).float()\n"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "8f8bdaf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T12:51:16.507312Z",
     "start_time": "2025-11-28T12:51:16.497566Z"
    }
   },
   "source": [
    "def prep_data(df):\n",
    "\n",
    "    all_values = df.values\n",
    "\n",
    "    tensors = torch.tensor(all_values, dtype = torch.float32)\n",
    "\n",
    "    raw_dist = tensors[:, 0]\n",
    "    raw_hours = tensors[:, 1]\n",
    "    raw_weekends=tensors[:, 2]\n",
    "    raw_targets = tensors[:, 3]\n",
    "\n",
    "    is_rush_hr = rush_hour_features(raw_hours, raw_weekends)  # Correct\n",
    "    dist = raw_dist.unsqueeze(1)\n",
    "    hrs = raw_hours.unsqueeze(1)\n",
    "    weeknd = raw_weekends.unsqueeze(1)\n",
    "    rush_hr = is_rush_hr\n",
    "\n",
    "    dist_std, dist_mean = dist.std(), dist.mean()\n",
    "    hrs_std, hrs_mean = hrs.std(), hrs.mean()\n",
    "\n",
    "    dist_norm = (dist - dist_mean)/ dist_std\n",
    "    hrs_norm = (hrs- hrs_mean)/ hrs_std\n",
    "\n",
    "    features = torch.cat([\n",
    "        dist_norm,\n",
    "        hrs_norm,\n",
    "        weeknd,\n",
    "        rush_hr],\n",
    "        dim = 1\n",
    "    )\n",
    "\n",
    "    targets = raw_targets.unsqueeze(1)\n",
    "\n",
    "    return_dict = {\n",
    "        'full_tensor': tensors,\n",
    "        'raw_distances': raw_dist,\n",
    "        'raw_hours': raw_hours,\n",
    "        'raw_weekends': raw_weekends,\n",
    "        'raw_targets': raw_targets,\n",
    "        'distances_col': dist,\n",
    "        'hours_col': hrs,\n",
    "        'weekends_col': weeknd,\n",
    "        'rush_hour_col': rush_hr\n",
    "    }\n",
    "\n",
    "    return features, targets, return_dict"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "7c1691ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T12:51:16.849462Z",
     "start_time": "2025-11-28T12:51:16.792789Z"
    }
   },
   "source": [
    "# Create a small test DataFrame with the first 5 entries\n",
    "test_df = data.head(5).copy()\n",
    "\n",
    "# Print the \"Before\" state as a raw tensor\n",
    "raw_test_tensor = torch.tensor(data.values, dtype=torch.float32)\n",
    "print(\"--- Raw Tensor (Before Preparation) ---\\n\")\n",
    "print(f\"Shape: {raw_test_tensor.shape}\")\n",
    "print(\"Values:\\n\", raw_test_tensor)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Run the function to get the prepared \"after\" tensors\n",
    "test_features, test_targets, _ = prep_data(test_df)\n",
    "\n",
    "# Print the \"After\" state\n",
    "print(\"--- Prepared Tensors (After Preparation) ---\")\n",
    "print(\"\\n--- Prepared Features ---\\n\")\n",
    "print(f\"Shape: {test_features.shape}\")\n",
    "print(\"Values:\\n\", test_features)\n",
    "\n",
    "print(\"\\n--- Prepared Targets ---\")\n",
    "print(f\"Shape: {test_targets.shape}\")\n",
    "print(\"Values:\\n\", test_targets)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Raw Tensor (Before Preparation) ---\n",
      "\n",
      "Shape: torch.Size([100, 4])\n",
      "Values:\n",
      " tensor([[ 1.6000,  8.2000,  0.0000,  7.2200],\n",
      "        [13.0900, 16.8000,  1.0000, 32.4100],\n",
      "        [ 6.9700,  8.0200,  1.0000, 17.4700],\n",
      "        [10.6600, 16.0700,  0.0000, 37.1700],\n",
      "        [18.2400, 13.4700,  0.0000, 38.3600],\n",
      "        [ 5.7400, 16.5900,  0.0000, 29.0600],\n",
      "        [ 8.8000, 12.2500,  0.0000, 23.9400],\n",
      "        [15.3600, 11.7600,  1.0000, 32.4000],\n",
      "        [ 5.3500,  9.4200,  0.0000, 17.0600],\n",
      "        [ 2.4600, 14.4400,  0.0000, 14.0900],\n",
      "        [ 6.5100,  8.0000,  0.0000, 33.3800],\n",
      "        [ 4.0600,  9.3300,  1.0000, 17.3800],\n",
      "        [18.6600, 14.8600,  1.0000, 36.7500],\n",
      "        [16.3500, 19.0900,  0.0000, 38.8600],\n",
      "        [13.0300, 13.4200,  0.0000, 32.5500],\n",
      "        [17.5600, 18.9200,  0.0000, 61.8700],\n",
      "        [16.2700, 15.2600,  0.0000, 38.0800],\n",
      "        [ 4.5400,  9.1000,  0.0000, 24.1200],\n",
      "        [17.9600, 15.2000,  0.0000, 43.2100],\n",
      "        [11.2500,  8.0000,  0.0000, 41.0300],\n",
      "        [16.3400,  8.3200,  0.0000, 59.6100],\n",
      "        [18.0300,  9.4900,  0.0000, 70.4600],\n",
      "        [ 7.0400,  8.0000,  0.0000, 31.5500],\n",
      "        [ 3.0900,  9.7000,  0.0000,  9.0600],\n",
      "        [ 5.3300, 19.5900,  0.0000,  8.2900],\n",
      "        [ 9.1200,  8.1700,  1.0000, 21.1800],\n",
      "        [16.5400,  8.0000,  0.0000, 61.1300],\n",
      "        [17.3500,  8.0000,  0.0000, 64.8400],\n",
      "        [ 1.1300, 17.1000,  0.0000,  7.7300],\n",
      "        [10.7000,  8.0000,  0.0000, 45.4200],\n",
      "        [ 8.9300,  9.8000,  0.0000, 36.5700],\n",
      "        [ 5.2200, 19.8200,  0.0000, 16.2400],\n",
      "        [ 3.2800,  8.0000,  0.0000, 12.4400],\n",
      "        [ 7.4100, 13.0100,  1.0000,  8.2500],\n",
      "        [18.9200, 17.5100,  1.0000, 42.4200],\n",
      "        [ 7.1400, 19.5400,  1.0000, 24.2800],\n",
      "        [10.8600,  8.0000,  0.0000, 41.9700],\n",
      "        [14.3600, 18.3800,  0.0000, 56.0000],\n",
      "        [ 7.9100, 13.9000,  0.0000, 21.7900],\n",
      "        [19.4600, 10.5800,  0.0000, 48.9500],\n",
      "        [19.2900, 19.0400,  0.0000, 46.2400],\n",
      "        [ 5.7800,  8.3200,  0.0000, 26.7300],\n",
      "        [10.4500, 10.5600,  0.0000, 26.0600],\n",
      "        [ 6.7200, 16.8400,  1.0000, 17.2700],\n",
      "        [ 6.4100, 14.7700,  0.0000, 13.5100],\n",
      "        [ 1.7000, 17.7300,  0.0000,  9.8000],\n",
      "        [12.5800, 18.1700,  0.0000, 51.6900],\n",
      "        [10.5500, 17.9400,  0.0000, 41.2300],\n",
      "        [ 1.9800, 17.9700,  0.0000, 12.0600],\n",
      "        [ 6.2900, 14.7900,  0.0000, 14.0600],\n",
      "        [18.2600, 13.3400,  1.0000, 39.2000],\n",
      "        [ 5.5500, 14.2100,  1.0000, 10.6100],\n",
      "        [ 3.7500, 14.8600,  1.0000, 12.6300],\n",
      "        [10.3000, 12.0300,  1.0000, 22.9800],\n",
      "        [19.7300, 15.4800,  0.0000, 39.2200],\n",
      "        [ 5.6000, 19.1300,  1.0000, 18.2900],\n",
      "        [13.7700,  8.0000,  0.0000, 52.7100],\n",
      "        [15.4700, 19.0400,  0.0000, 41.5200],\n",
      "        [ 5.5200,  8.0000,  0.0000, 28.7400],\n",
      "        [14.8400, 19.2200,  0.0000, 32.0300],\n",
      "        [ 7.9900,  9.2100,  0.0000, 30.5800],\n",
      "        [13.0100, 17.1800,  0.0000, 47.0300],\n",
      "        [13.0400,  8.4500,  1.0000, 23.7100],\n",
      "        [11.1800, 18.6800,  0.0000, 42.8600],\n",
      "        [ 2.7200, 15.6200,  0.0000, 14.7600],\n",
      "        [16.8700,  8.0000,  0.0000, 58.3000],\n",
      "        [ 7.0900, 13.0700,  0.0000, 21.7500],\n",
      "        [ 4.5400, 16.7300,  1.0000, 15.4800],\n",
      "        [ 1.7700, 14.1800,  0.0000, 10.2100],\n",
      "        [12.2300, 17.7000,  1.0000, 29.2100],\n",
      "        [13.8700, 12.1600,  1.0000, 26.6600],\n",
      "        [ 1.3200, 19.2900,  0.0000,  9.9200],\n",
      "        [10.7300, 13.1100,  0.0000, 24.8100],\n",
      "        [ 5.3000, 13.6900,  1.0000,  7.1700],\n",
      "        [13.2600, 19.6200,  0.0000, 31.6900],\n",
      "        [ 4.3100, 14.2100,  1.0000, 10.2300],\n",
      "        [14.1300,  8.0000,  1.0000, 34.5100],\n",
      "        [ 8.3500, 17.4800,  0.0000, 33.3600],\n",
      "        [18.8000, 15.3400,  0.0000, 42.7300],\n",
      "        [ 3.6100, 19.4700,  0.0000, 13.3900],\n",
      "        [ 7.4800,  9.2800,  1.0000, 17.6600],\n",
      "        [ 3.1600,  9.3800,  0.0000, 15.7300],\n",
      "        [18.5700,  8.0100,  0.0000, 72.5200],\n",
      "        [17.6700, 16.2800,  0.0000, 63.6400],\n",
      "        [ 5.9000, 15.9400,  0.0000, 14.7000],\n",
      "        [13.5400, 19.7700,  0.0000, 29.1800],\n",
      "        [16.5300, 14.1800,  1.0000, 39.0600],\n",
      "        [11.5500, 19.0300,  0.0000, 25.9900],\n",
      "        [11.0600, 10.8400,  1.0000, 24.4800],\n",
      "        [ 5.6000,  8.6500,  0.0000, 22.7800],\n",
      "        [ 2.7700,  9.3000,  0.0000, 13.7700],\n",
      "        [18.0500, 17.8900,  0.0000, 65.9500],\n",
      "        [18.1100,  9.3000,  1.0000, 38.9800],\n",
      "        [13.0300,  8.4700,  0.0000, 47.4800],\n",
      "        [ 7.4400,  8.0000,  1.0000, 14.3400],\n",
      "        [ 7.6300, 16.5300,  0.0000, 30.0500],\n",
      "        [14.7900,  8.0000,  0.0000, 54.3800],\n",
      "        [18.0500,  8.0000,  0.0000, 64.1400],\n",
      "        [17.8500, 19.3600,  0.0000, 39.7400],\n",
      "        [15.8200, 19.5200,  0.0000, 31.6500]])\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Prepared Tensors (After Preparation) ---\n",
      "\n",
      "--- Prepared Features ---\n",
      "\n",
      "Shape: torch.Size([5, 4])\n",
      "Values:\n",
      " tensor([[-1.3562, -1.0254,  0.0000,  1.0000],\n",
      "        [ 0.4745,  1.0197,  1.0000,  0.0000],\n",
      "        [-0.5006, -1.0682,  1.0000,  0.0000],\n",
      "        [ 0.0873,  0.8461,  0.0000,  1.0000],\n",
      "        [ 1.2951,  0.2278,  0.0000,  0.0000]])\n",
      "\n",
      "--- Prepared Targets ---\n",
      "Shape: torch.Size([5, 1])\n",
      "Values:\n",
      " tensor([[ 7.2200],\n",
      "        [32.4100],\n",
      "        [17.4700],\n",
      "        [37.1700],\n",
      "        [38.3600]])\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "e9cfce21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T12:51:17.080933Z",
     "start_time": "2025-11-28T12:51:17.074487Z"
    }
   },
   "source": [
    "features, targets, _ = prep_data(data)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "f6861304",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T12:51:17.240064Z",
     "start_time": "2025-11-28T12:51:17.231961Z"
    }
   },
   "source": [
    "def new_model():\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(4,64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64,32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32,1)\n",
    "    )\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    loss_function = nn.MSELoss()\n",
    "\n",
    "\n",
    "    return model, optimizer, loss_function"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "fb357841",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T12:51:17.448615Z",
     "start_time": "2025-11-28T12:51:17.428709Z"
    }
   },
   "source": [
    "model, optimizer, loss_function = new_model()\n",
    "\n",
    "print(f\"{'='*30}\\nInitialized Model Architecture\\n{'='*30}\\n{model}\")\n",
    "print(f\"\\n{'='*30}\\nOptimizer\\n{'='*30}\\n{optimizer}\")\n",
    "print(f\"\\n{'='*30}\\nLoss Function\\n{'='*30}\\n{loss_function}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Initialized Model Architecture\n",
      "==============================\n",
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "==============================\n",
      "Optimizer\n",
      "==============================\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "\n",
      "==============================\n",
      "Loss Function\n",
      "==============================\n",
      "MSELoss()\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "d511d5c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T12:51:17.734977Z",
     "start_time": "2025-11-28T12:51:17.726149Z"
    }
   },
   "source": [
    "def tarin(features, targets, epochs, verbose = True):\n",
    "\n",
    "    losses= []\n",
    "\n",
    "    model, optimizer, loss_function = new_model()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        output = model(features)\n",
    "        loss = loss_function(output, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "         # Every 5000 epochs, record the loss and print the progress\n",
    "        if (epoch + 1) % 5000 == 0:\n",
    "            losses.append(loss.item())\n",
    "            if verbose:\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return model, losses"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "5902a34e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T12:51:35.794899Z",
     "start_time": "2025-11-28T12:51:17.909508Z"
    }
   },
   "source": [
    "test_model = tarin(features, targets,10000)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5000/10000], Loss: 4.0634\n",
      "Epoch [10000/10000], Loss: 1.9242\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "5f303394",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-11-28T12:51:35.909640Z"
    }
   },
   "source": [
    "# Training loop\n",
    "model, loss = tarin(features, targets, 30000)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5000/30000], Loss: 4.2364\n",
      "Epoch [10000/30000], Loss: 2.9529\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[19]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Training loop\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m model, loss = \u001B[43mtarin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m30000\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[17]\u001B[39m\u001B[32m, line 8\u001B[39m, in \u001B[36mtarin\u001B[39m\u001B[34m(features, targets, epochs, verbose)\u001B[39m\n\u001B[32m      5\u001B[39m model, optimizer, loss_function = new_model()\n\u001B[32m      7\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m     output = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      9\u001B[39m     loss = loss_function(output, targets)\n\u001B[32m     11\u001B[39m     optimizer.zero_grad()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001B[39m, in \u001B[36mSequential.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    246\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    247\u001B[39m \u001B[33;03mRuns the forward pass.\u001B[39;00m\n\u001B[32m    248\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    249\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m250\u001B[39m     \u001B[38;5;28minput\u001B[39m = \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    251\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001B[39m, in \u001B[36mLinear.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    130\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m    131\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    132\u001B[39m \u001B[33;03m    Runs the forward pass.\u001B[39;00m\n\u001B[32m    133\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m134\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d63aeba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable gradient calculation for efficient predictions\n",
    "with torch.no_grad():\n",
    "    # Perform a forward pass to get model predictions\n",
    "    predicted_outputs = model(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1153f42c",
   "metadata": {},
   "source": [
    "*DAY 2*\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "acf02e0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T05:33:16.837053Z",
     "start_time": "2025-11-29T05:33:04.796936Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "00fd21cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T05:33:16.921543Z",
     "start_time": "2025-11-29T05:33:16.893194Z"
    }
   },
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using device: CUDA\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"Using device: MPS (Apple Silicon GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Using device: CPU\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: CPU\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "a679eb7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T05:33:17.027602Z",
     "start_time": "2025-11-29T05:33:17.018810Z"
    }
   },
   "source": [
    "path = \"./datasets\""
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "389af385",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T05:33:17.193980Z",
     "start_time": "2025-11-29T05:33:17.060764Z"
    }
   },
   "source": [
    "train_dataset_no_trsfm = torchvision.datasets.MNIST(\n",
    "    root = path,\n",
    "    train = True,\n",
    "    download = True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "93e5fcdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T05:33:17.268727Z",
     "start_time": "2025-11-29T05:33:17.228164Z"
    }
   },
   "source": [
    "image_pil, label = train_dataset_no_trsfm[0]\n",
    "\n",
    "print(f\"Image type:    {type(image_pil)}\")\n",
    "print(f\"Image Dimension: {image_pil.size}\")\n",
    "print(f\"Label type:  {type(label)}\")\n",
    "print(f\"Label Vlaue:  {label}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image type:    <class 'PIL.Image.Image'>\n",
      "Image Dimension: (28, 28)\n",
      "Label type:  <class 'int'>\n",
      "Label Vlaue:  5\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "0197b8f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T05:33:17.371568Z",
     "start_time": "2025-11-29T05:33:17.364323Z"
    }
   },
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.01307), (0.3081))\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "dce96972",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T05:33:17.488804Z",
     "start_time": "2025-11-29T05:33:17.412651Z"
    }
   },
   "source": [
    "traind_data= torchvision.datasets.MNIST(\n",
    "    root = path,\n",
    "    train = True,\n",
    "    download  = True,\n",
    "    transform = transform\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "b16fbe25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T05:33:17.548635Z",
     "start_time": "2025-11-29T05:33:17.508452Z"
    }
   },
   "source": [
    "# Access the first item again\n",
    "image_tensor, label = traind_data[0]\n",
    "\n",
    "print(f\"Image Type:                   {type(image_tensor)}\")\n",
    "# Since the `image` is now a PyTorch Tensor, its dimensions are accessed using the .shape attribute.\n",
    "print(f\"Image Shape After Transform:  {image_tensor.shape}\")\n",
    "print(f\"Label Type:                   {type(label)}\")\n",
    "print(f\"Label value:                  {label}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Type:                   <class 'torch.Tensor'>\n",
      "Image Shape After Transform:  torch.Size([1, 28, 28])\n",
      "Label Type:                   <class 'int'>\n",
      "Label value:                  5\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "0cb1f12d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T05:33:17.614563Z",
     "start_time": "2025-11-29T05:33:17.586558Z"
    }
   },
   "source": [
    "test_data = torchvision.datasets.MNIST(\n",
    "    root = path,\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = transform\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "5c7f4b0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T05:33:17.635645Z",
     "start_time": "2025-11-29T05:33:17.627732Z"
    }
   },
   "source": [
    "train_dt_ldr = DataLoader(traind_data, batch_size = 32, shuffle = True)\n",
    "\n",
    "test_dt_ldr = DataLoader(test_data, batch_size = 64, shuffle = False)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "afc6fea93953724a"
  },
  {
   "cell_type": "code",
   "id": "407b49bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T05:36:53.343523Z",
     "start_time": "2025-11-29T05:36:53.336208Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "class SimpleMNISTNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleMNISTNN, self).__init__()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(784,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T05:36:53.805362Z",
     "start_time": "2025-11-29T05:36:53.799348Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "96b87e268f6d2e42",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d4f7c761",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T05:36:54.508886Z",
     "start_time": "2025-11-29T05:36:54.495851Z"
    }
   },
   "source": [
    "model = SimpleMNISTNN()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "eddfc675",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T06:02:35.799900Z",
     "start_time": "2025-11-29T06:02:35.783923Z"
    }
   },
   "source": [
    "def train_epoches(model, loss_function, optimizer, train_loader, device):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    total_preds = 0\n",
    "    num_correct_preds =0\n",
    "    total_batches = len(train_loader)\n",
    "\n",
    "    for batch_idx, (input, targets) in enumerate(train_loader):\n",
    "\n",
    "        input, targets = input.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(input)\n",
    "        loss = loss_function(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_valu = loss.item()\n",
    "        epoch_loss +=loss_valu\n",
    "        running_loss += loss_valu\n",
    "\n",
    "        _, predicted_indices = output.max(1)\n",
    "        batch_size = targets.size(0)\n",
    "        total_preds += batch_size\n",
    "        num_correct_preds += predicted_indices.eq(targets).sum().item()\n",
    "\n",
    "        if (batch_idx + 1) % 134 == 0 or (batch_idx + 1) == total_batches:\n",
    "            # Calculate average loss and accuracy for the current interval\n",
    "            avg_running_loss = running_loss / 134\n",
    "            accuracy = 100 * (num_correct_preds / total_preds)\n",
    "\n",
    "            # Print the progress update\n",
    "            print(f'\\tStep {batch_idx + 1}/{total_batches} - Loss: {avg_running_loss:.3f} | Acc: {accuracy:.2f}%')\n",
    "\n",
    "            # Reset the trackers for the next reporting interval\n",
    "            running_loss = 0.0\n",
    "            num_correct_preds = 0\n",
    "            total_preds = 0\n",
    "\n",
    "    # Calculate the average loss for the entire epoch\n",
    "    avg_epoch_loss = epoch_loss / total_batches\n",
    "    # Return the trained model and the average epoch loss\n",
    "    return model, avg_epoch_loss\n"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T06:10:07.977313Z",
     "start_time": "2025-11-29T06:10:07.968291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(model, test_loader, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    num_crct_preds = 0\n",
    "    total_predfs = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            output = model(inputs)\n",
    "            _, predicted_indices = torch.max(output, 1)\n",
    "            batch_size = targets.size(0)\n",
    "            total_predfs += batch_size\n",
    "            num_crct_preds += predicted_indices.eq(targets).sum().item()\n",
    "\n",
    "        accuracy_percent = (num_crct_preds/total_predfs)*100\n",
    "        print((f'\\tAccuracy - {accuracy_percent:.2f}%'))\n",
    "\n",
    "        return accuracy_percent"
   ],
   "id": "7e4410b56922564",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T06:13:44.133993Z",
     "start_time": "2025-11-29T06:10:08.260630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 5\n",
    "\n",
    "\n",
    "train_losss = []\n",
    "test_acc = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "\n",
    "    trained_model, loss = train_epoches(model, loss_function, optimizer, train_dt_ldr, device)\n",
    "\n",
    "    train_losss.append(loss)\n",
    "    print(\n",
    "         f\"Testing Epoch {epoch+1}\")\n",
    "\n",
    "    accuracy = evaluate(trained_model, test_dt_ldr, device)\n",
    "    test_acc.append(accuracy)"
   ],
   "id": "a175fa39560b8323",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\tStep 134/1875 - Loss: 0.062 | Acc: 98.34%\n",
      "\tStep 268/1875 - Loss: 0.059 | Acc: 97.95%\n",
      "\tStep 402/1875 - Loss: 0.068 | Acc: 97.74%\n",
      "\tStep 536/1875 - Loss: 0.070 | Acc: 97.90%\n",
      "\tStep 670/1875 - Loss: 0.064 | Acc: 97.90%\n",
      "\tStep 804/1875 - Loss: 0.068 | Acc: 97.71%\n",
      "\tStep 938/1875 - Loss: 0.066 | Acc: 97.85%\n",
      "\tStep 1072/1875 - Loss: 0.060 | Acc: 98.04%\n",
      "\tStep 1206/1875 - Loss: 0.064 | Acc: 97.99%\n",
      "\tStep 1340/1875 - Loss: 0.062 | Acc: 97.99%\n",
      "\tStep 1474/1875 - Loss: 0.071 | Acc: 97.55%\n",
      "\tStep 1608/1875 - Loss: 0.064 | Acc: 97.85%\n",
      "\tStep 1742/1875 - Loss: 0.067 | Acc: 97.81%\n",
      "\tStep 1875/1875 - Loss: 0.057 | Acc: 98.10%\n",
      "Testing Epoch 1\n",
      "\tAccuracy - 97.34%\n",
      "Epoch 2\n",
      "\tStep 134/1875 - Loss: 0.046 | Acc: 98.48%\n",
      "\tStep 268/1875 - Loss: 0.040 | Acc: 98.60%\n",
      "\tStep 402/1875 - Loss: 0.053 | Acc: 98.44%\n",
      "\tStep 536/1875 - Loss: 0.057 | Acc: 98.39%\n",
      "\tStep 670/1875 - Loss: 0.043 | Acc: 98.32%\n",
      "\tStep 804/1875 - Loss: 0.057 | Acc: 98.16%\n",
      "\tStep 938/1875 - Loss: 0.044 | Acc: 98.48%\n",
      "\tStep 1072/1875 - Loss: 0.050 | Acc: 98.41%\n",
      "\tStep 1206/1875 - Loss: 0.050 | Acc: 98.51%\n",
      "\tStep 1340/1875 - Loss: 0.042 | Acc: 98.55%\n",
      "\tStep 1474/1875 - Loss: 0.057 | Acc: 98.30%\n",
      "\tStep 1608/1875 - Loss: 0.051 | Acc: 98.39%\n",
      "\tStep 1742/1875 - Loss: 0.067 | Acc: 97.99%\n",
      "\tStep 1875/1875 - Loss: 0.046 | Acc: 98.36%\n",
      "Testing Epoch 2\n",
      "\tAccuracy - 97.07%\n",
      "Epoch 3\n",
      "\tStep 134/1875 - Loss: 0.035 | Acc: 98.90%\n",
      "\tStep 268/1875 - Loss: 0.041 | Acc: 98.65%\n",
      "\tStep 402/1875 - Loss: 0.030 | Acc: 99.00%\n",
      "\tStep 536/1875 - Loss: 0.034 | Acc: 98.88%\n",
      "\tStep 670/1875 - Loss: 0.039 | Acc: 98.60%\n",
      "\tStep 804/1875 - Loss: 0.040 | Acc: 98.65%\n",
      "\tStep 938/1875 - Loss: 0.041 | Acc: 98.55%\n",
      "\tStep 1072/1875 - Loss: 0.048 | Acc: 98.34%\n",
      "\tStep 1206/1875 - Loss: 0.032 | Acc: 99.04%\n",
      "\tStep 1340/1875 - Loss: 0.040 | Acc: 98.93%\n",
      "\tStep 1474/1875 - Loss: 0.037 | Acc: 98.67%\n",
      "\tStep 1608/1875 - Loss: 0.047 | Acc: 98.39%\n",
      "\tStep 1742/1875 - Loss: 0.047 | Acc: 98.39%\n",
      "\tStep 1875/1875 - Loss: 0.052 | Acc: 98.31%\n",
      "Testing Epoch 3\n",
      "\tAccuracy - 97.92%\n",
      "Epoch 4\n",
      "\tStep 134/1875 - Loss: 0.019 | Acc: 99.44%\n",
      "\tStep 268/1875 - Loss: 0.020 | Acc: 99.25%\n",
      "\tStep 402/1875 - Loss: 0.033 | Acc: 99.04%\n",
      "\tStep 536/1875 - Loss: 0.022 | Acc: 99.42%\n",
      "\tStep 670/1875 - Loss: 0.038 | Acc: 98.60%\n",
      "\tStep 804/1875 - Loss: 0.029 | Acc: 98.88%\n",
      "\tStep 938/1875 - Loss: 0.034 | Acc: 98.93%\n",
      "\tStep 1072/1875 - Loss: 0.034 | Acc: 98.90%\n",
      "\tStep 1206/1875 - Loss: 0.045 | Acc: 98.65%\n",
      "\tStep 1340/1875 - Loss: 0.033 | Acc: 99.02%\n",
      "\tStep 1474/1875 - Loss: 0.032 | Acc: 98.88%\n",
      "\tStep 1608/1875 - Loss: 0.033 | Acc: 98.81%\n",
      "\tStep 1742/1875 - Loss: 0.037 | Acc: 98.65%\n",
      "\tStep 1875/1875 - Loss: 0.032 | Acc: 98.97%\n",
      "Testing Epoch 4\n",
      "\tAccuracy - 97.61%\n",
      "Epoch 5\n",
      "\tStep 134/1875 - Loss: 0.021 | Acc: 99.21%\n",
      "\tStep 268/1875 - Loss: 0.013 | Acc: 99.60%\n",
      "\tStep 402/1875 - Loss: 0.019 | Acc: 99.37%\n",
      "\tStep 536/1875 - Loss: 0.017 | Acc: 99.44%\n",
      "\tStep 670/1875 - Loss: 0.021 | Acc: 99.30%\n",
      "\tStep 804/1875 - Loss: 0.032 | Acc: 99.00%\n",
      "\tStep 938/1875 - Loss: 0.029 | Acc: 98.90%\n",
      "\tStep 1072/1875 - Loss: 0.044 | Acc: 98.65%\n",
      "\tStep 1206/1875 - Loss: 0.037 | Acc: 98.76%\n",
      "\tStep 1340/1875 - Loss: 0.027 | Acc: 99.04%\n",
      "\tStep 1474/1875 - Loss: 0.025 | Acc: 99.18%\n",
      "\tStep 1608/1875 - Loss: 0.036 | Acc: 98.65%\n",
      "\tStep 1742/1875 - Loss: 0.023 | Acc: 99.25%\n",
      "\tStep 1875/1875 - Loss: 0.037 | Acc: 98.75%\n",
      "Testing Epoch 5\n",
      "\tAccuracy - 97.77%\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4f893427d096a7c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#Day 3#",
   "id": "6b9734ae7f5d0d33"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T14:39:57.128432Z",
     "start_time": "2025-12-01T14:39:53.009909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import tarfile\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, Subset, random_split, DataLoader\n",
    "from torchvision import transforms\n"
   ],
   "id": "356989a573b3433e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T14:39:57.160891Z",
     "start_time": "2025-12-01T14:39:57.149850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "from tqdm import tqdm  # <-- This was missing!\n",
    "\n",
    "def download_dataset():\n",
    "    \"\"\"\n",
    "    Downloads and extracts the Oxford 102 Flower dataset if not already present locally.\n",
    "    Source: https://www.robots.ox.ac.uk/~vgg/data/flowers/102/\n",
    "    \"\"\"\n",
    "    # Define the directory to store the dataset\n",
    "    data_dir = \"flower_data\"\n",
    "    image_folder_path = os.path.join(data_dir, \"jpg\")\n",
    "    labels_file_path = os.path.join(data_dir, \"imagelabels.mat\")\n",
    "    tgz_path = os.path.join(data_dir, \"102flowers.tgz\")\n",
    "\n",
    "    # Check if dataset already exists\n",
    "    if os.path.exists(image_folder_path) and os.path.exists(labels_file_path):\n",
    "        print(f\"Dataset already exists. Loading locally from '{data_dir}'.\")\n",
    "        return\n",
    "\n",
    "    print(\"Dataset not found locally. Downloading...\")\n",
    "\n",
    "    # URLs\n",
    "    image_url = \"https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\"\n",
    "    labels_url = \"https://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat\"\n",
    "\n",
    "    # Create directory\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    # Download images with progress bar\n",
    "    print(\"Downloading images (~330 MB)...\")\n",
    "    response = requests.get(image_url, stream=True)\n",
    "    response.raise_for_status()  # Raise an exception for bad status\n",
    "    total_size = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "    with open(tgz_path, \"wb\") as file:\n",
    "        # tqdm progress bar (note: unit='KB' for clarity)\n",
    "        for data in tqdm(response.iter_content(chunk_size=1024),\n",
    "                         total=total_size // 1024,\n",
    "                         unit='KB',\n",
    "                         unit_scale=True):\n",
    "            file.write(data)\n",
    "\n",
    "    # Extract tar.gz\n",
    "    print(\"Extracting images...\")\n",
    "    with tarfile.open(tgz_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=data_dir)\n",
    "\n",
    "    # Download labels\n",
    "    print(\"Downloading labels...\")\n",
    "    response = requests.get(labels_url)\n",
    "    response.raise_for_status()\n",
    "    with open(labels_file_path, \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "    print(f\"Dataset successfully downloaded and extracted to '{data_dir}'.\")\n",
    "\n",
    "# Run it\n",
    "download_dataset()"
   ],
   "id": "7fc6a8b6c05e86f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists. Loading locally from 'flower_data'.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T14:40:01.072023Z",
     "start_time": "2025-12-01T14:40:01.065899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the path to the root directory of the dataset.\n",
    "path_dataset = './flower_data'\n",
    "\n",
    "# Display the folder structure of the dataset directory up to a depth of one."
   ],
   "id": "1e298a54b70303fb",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T14:40:01.751204Z",
     "start_time": "2025-12-01T14:40:01.737477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FlowerDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_dir = os.path.join(root_dir, 'jpg')\n",
    "        self.labels = self.load_and_correct_labels()\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.retrieve_image(idx)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "\n",
    "    def retrieve_image(self ,idx):\n",
    "        img_name = f\"image_{idx+1:05d}.jpg\"\n",
    "\n",
    "        image_path = os.path.join(self.image_dir, img_name)\n",
    "\n",
    "        with Image.open(image_path) as img:\n",
    "            image = img.convert(\"RGB\")\n",
    "        return image\n",
    "    def load_and_correct_labels(self):\n",
    "        self.labels_mat = scipy.io.loadmat(\n",
    "            os.path.join(self.root_dir, 'imagelabels.mat')\n",
    "        )\n",
    "\n",
    "        labels = self.labels_mat['labels'][0]-1\n",
    "        return labels\n",
    "\n",
    "    def get_label_description(self,label):\n",
    "        path_labels_description = os.path.join(self.root_dir, 'label_description.txt')\n",
    "\n",
    "        with open(path_labels_description, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        description = lines[label].strip()\n",
    "\n",
    "        return description\n"
   ],
   "id": "3dbeb579dcaef3de",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T14:40:02.417533Z",
     "start_time": "2025-12-01T14:40:02.264749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the dataset object, providing the path to the data.\n",
    "dataset = FlowerDataset(path_dataset)"
   ],
   "id": "eabd0a73c0b4cafe",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T14:40:03.069621Z",
     "start_time": "2025-12-01T14:40:03.062698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print the total number of samples in the dataset.\n",
    "print(f'Number of samples in the dataset: {len(dataset)}\\n')"
   ],
   "id": "806c65d5a5a667",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the dataset: 8189\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T14:40:03.833522Z",
     "start_time": "2025-12-01T14:40:03.776483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define an index for a sample to retrieve.\n",
    "sel_idx = 10\n",
    "\n",
    "# Retrieve the image and label for the selected index.\n",
    "img, label = dataset[sel_idx]"
   ],
   "id": "32283c5f781b04",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T14:40:04.407159Z",
     "start_time": "2025-12-01T14:40:04.400759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a string detailing the image's dimensions.\n",
    "img_size_info = f\"Image size: {img.size}\"\n",
    "\n",
    "# Print the image size information along with its corresponding label.\n",
    "print(f'{img_size_info}, Label: {label}\\n')"
   ],
   "id": "b671667fe367395",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: (500, 748), Label: 76\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T14:40:05.522249Z",
     "start_time": "2025-12-01T14:40:05.094358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get all labels from the dataset object.\n",
    "dataset_labels = dataset.labels\n",
    "\n",
    "# Create a set of unique labels to remove duplicates.\n",
    "unique_labels = set(dataset_labels)\n",
    "\n",
    "# Iterate through each unique label.\n",
    "for label in unique_labels:\n",
    "    # Print the numerical label and its corresponding text description.\n",
    "    print(f'Label: {label}, Description: {dataset.get_label_description(label)}')"
   ],
   "id": "a79e35259668216f",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './flower_data\\\\label_description.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 10\u001B[39m\n\u001B[32m      7\u001B[39m \u001B[38;5;66;03m# Iterate through each unique label.\u001B[39;00m\n\u001B[32m      8\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m label \u001B[38;5;129;01min\u001B[39;00m unique_labels:\n\u001B[32m      9\u001B[39m     \u001B[38;5;66;03m# Print the numerical label and its corresponding text description.\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mLabel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlabel\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, Description: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[43mdataset\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_label_description\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 41\u001B[39m, in \u001B[36mFlowerDataset.get_label_description\u001B[39m\u001B[34m(self, label)\u001B[39m\n\u001B[32m     38\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mget_label_description\u001B[39m(\u001B[38;5;28mself\u001B[39m,label):\n\u001B[32m     39\u001B[39m     path_labels_description = os.path.join(\u001B[38;5;28mself\u001B[39m.root_dir, \u001B[33m'\u001B[39m\u001B[33mlabel_description.txt\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m41\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mpath_labels_description\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mr\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[32m     42\u001B[39m         lines = f.readlines()\n\u001B[32m     43\u001B[39m     description = lines[label].strip()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\interactiveshell.py:343\u001B[39m, in \u001B[36m_modified_open\u001B[39m\u001B[34m(file, *args, **kwargs)\u001B[39m\n\u001B[32m    336\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[32m0\u001B[39m, \u001B[32m1\u001B[39m, \u001B[32m2\u001B[39m}:\n\u001B[32m    337\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    338\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mIPython won\u001B[39m\u001B[33m'\u001B[39m\u001B[33mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m by default \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    339\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    340\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33myou can use builtins\u001B[39m\u001B[33m'\u001B[39m\u001B[33m open.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    341\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m343\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: './flower_data\\\\label_description.txt'"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T14:40:10.809912Z",
     "start_time": "2025-12-01T14:40:10.799626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "def visual_exploration(dataset, num_rows=2, num_cols=4):\n",
    "    \"\"\"\n",
    "    Displays a grid of randomly selected samples from a dataset for visual inspection.\n",
    "\n",
    "    Args:\n",
    "        dataset: The dataset object from which to draw samples. It should support\n",
    "                 indexing and have a `get_label_description` method.\n",
    "        num_rows (int): The number of rows in the display grid.\n",
    "        num_cols (int): The number of columns in the display grid.\n",
    "    \"\"\"\n",
    "    # Calculate the total number of images to display in the grid.\n",
    "    total_samples = num_rows * num_cols\n",
    "\n",
    "    # Select a random set of unique indices from the dataset.\n",
    "    indices = np.random.choice(len(dataset), total_samples, replace=False)\n",
    "\n",
    "    # Create a grid of subplots to hold the images.\n",
    "    fig, axes = helper_utils.get_grid(num_rows, num_cols, figsize=(num_cols * 3, num_rows * 4))\n",
    "\n",
    "    # Iterate over each subplot axis and the corresponding random sample index.\n",
    "    for ax, idx in zip(axes.flatten(), indices):\n",
    "        # Retrieve the image and its numerical label from the dataset.\n",
    "        image, label = dataset[idx]\n",
    "\n",
    "        # Get the human-readable text description for the label.\n",
    "        description = dataset.get_label_description(label)\n",
    "\n",
    "        # Format a new label string that includes both the number and description.\n",
    "        label = f\"{label} - {description}\"\n",
    "\n",
    "        # Create an information string with the sample's index and image dimensions.\n",
    "        info = f\"Index: {idx} Size: {image.size}\"\n",
    "\n",
    "        # Plot the image on the current subplot with its label and info.\n",
    "        helper_utils.plot_img(image, label=label, info=info, ax=ax)\n",
    "\n",
    "    # Render and display the entire grid of images.\n",
    "    plt.show()"
   ],
   "id": "214da3d11514ff0e",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "964a48763260d041"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T14:40:12.370700Z",
     "start_time": "2025-12-01T14:40:12.365651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the mean values for normalization.\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "# Define the standard deviation values for normalization.\n",
    "std = [0.229, 0.224, 0.225]"
   ],
   "id": "9e5f9919eadad1f9",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T14:40:13.428684Z",
     "start_time": "2025-12-01T14:40:13.402623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([\n",
    "    # images transforms\n",
    "    transforms.Resize((256, 256)),  # Resize images to 256x256 pixels\n",
    "    transforms.CenterCrop(224),  # Center crop to 224x224 pixels\n",
    "    # bridge to tensor\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    # tensor transforms\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])"
   ],
   "id": "9a2df5ba3a92144b",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T14:40:14.637022Z",
     "start_time": "2025-12-01T14:40:14.627172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a new dataset instance with the specified image transformations.\n",
    "dataset_transformed = FlowerDataset(path_dataset, transform=transform)"
   ],
   "id": "85f95d4eb8ef4e4a",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T14:40:58.379819Z",
     "start_time": "2025-12-01T14:40:58.370498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Denormalize:\n",
    "    \"\"\"\n",
    "    A callable class to reverse the normalization of a tensor image.\n",
    "\n",
    "    This class calculates the inverse transformation of a standard normalization\n",
    "    and can be used as a transform step, for instance, to visualize images\n",
    "    after they have been normalized for a model.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean, std):\n",
    "        \"\"\"\n",
    "        Initializes the denormalization transform.\n",
    "\n",
    "        Args:\n",
    "            mean (list or tuple): The mean values used for the original normalization.\n",
    "            std (list or tuple): The standard deviation values used for the original\n",
    "                                 normalization.\n",
    "        \"\"\"\n",
    "        # Calculate the adjusted mean for the denormalization process.\n",
    "        new_mean = [-m / s for m, s in zip(mean, std)]\n",
    "        # Calculate the adjusted standard deviation for the denormalization process.\n",
    "        new_std = [1 / s for s in std]\n",
    "        # Create a Normalize transform object with the inverse parameters.\n",
    "        self.denormalize = transforms.Normalize(mean=new_mean, std=new_std)\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Applies the denormalization transform to a tensor.\n",
    "\n",
    "        Args:\n",
    "            tensor: The normalized tensor to be denormalized.\n",
    "\n",
    "        Returns:\n",
    "            The denormalized tensor.\n",
    "        \"\"\"\n",
    "        # Apply the denormalization transform to the input tensor.\n",
    "        return self.denormalize(tensor)"
   ],
   "id": "4c5b5b45f6a1a422",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T14:41:04.033878Z",
     "start_time": "2025-12-01T14:41:03.890643Z"
    }
   },
   "cell_type": "code",
   "source": "img_transformed, label = dataset_transformed[sel_idx]\n",
   "id": "52347672d5483bd9",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T14:41:04.656599Z",
     "start_time": "2025-12-01T14:41:04.649461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create an instance of the Denormalize class with the original mean and std.\n",
    "denormalize = Denormalize(mean=mean, std=std)\n",
    "# Apply the denormalization transform to the image tensor.\n",
    "img_tensor = denormalize(img_transformed)"
   ],
   "id": "3bb6ef02d0798a2f",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T14:41:35.315726Z",
     "start_time": "2025-12-01T14:41:35.307653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split_dataset(dataset, val_fraction=0.15, test_fraction=0.15):\n",
    "    \"\"\"\n",
    "    Split the dataset into training, validation, and test sets.\n",
    "\n",
    "    By default, this function splits the data into 70% for training,\n",
    "    15% for validation, and 15% for testing.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the sizes of each split.\n",
    "    total_size = len(dataset)\n",
    "    val_size = int(total_size * val_fraction)\n",
    "    test_size = int(total_size * test_fraction)\n",
    "    train_size = total_size - val_size - test_size\n",
    "\n",
    "    # Use random_split to create the datasets.\n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        dataset, [train_size, val_size, test_size]\n",
    "    )\n",
    "    return train_dataset, val_dataset, test_dataset"
   ],
   "id": "bbbfc753cc8f8771",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T14:41:40.432800Z",
     "start_time": "2025-12-01T14:41:40.383295Z"
    }
   },
   "cell_type": "code",
   "source": "train_dataset, val_dataset, test_dataset = split_dataset(dataset_transformed)",
   "id": "a50943eee8e32af3",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T14:41:45.111226Z",
     "start_time": "2025-12-01T14:41:45.104264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Length of training dataset:   {len(train_dataset)}\")\n",
    "print(f\"Length of validation dataset: {len(val_dataset)}\")\n",
    "print(f\"Length of test dataset:       {len(test_dataset)}\")"
   ],
   "id": "3317c7920d1aed1d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training dataset:   5733\n",
      "Length of validation dataset: 1228\n",
      "Length of test dataset:       1228\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T14:41:53.707563Z",
     "start_time": "2025-12-01T14:41:53.694738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set the batch size for the data loaders.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for the training set, with shuffling enabled.\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create the DataLoader for the validation set, with shuffling disabled.\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Create the DataLoader for the test set, with shuffling disabled.\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "3ecc92fba36ad794",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T14:42:07.433436Z",
     "start_time": "2025-12-01T14:42:07.427026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_augmentation_transform(mean, std):\n",
    "    \"\"\"\n",
    "    Creates and returns a composition of image transformations for data augmentation\n",
    "    and preprocessing.\n",
    "\n",
    "    Args:\n",
    "        mean (list or tuple): A sequence of mean values for each channel.\n",
    "        std (list or tuple): A sequence of standard deviation values for each channel.\n",
    "\n",
    "    Returns:\n",
    "        torchvision.transforms.Compose: A composed pipeline of transformations.\n",
    "    \"\"\"\n",
    "    # Define a list of data augmentation transformations to be applied randomly.\n",
    "    augmentations_transforms = [\n",
    "        # Randomly flip the image horizontally with a 50% probability.\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        # Randomly rotate the image within a range of +/- 10 degrees.\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        # Randomly adjust the brightness of the image.\n",
    "        transforms.ColorJitter(brightness=0.2),\n",
    "    ]\n",
    "\n",
    "    # Define the main list of standard, non-random transformations.\n",
    "    main_transforms = [\n",
    "        # Resize the input image to 256x256 pixels.\n",
    "        transforms.Resize((256, 256)),\n",
    "        # Crop the center 224x224 pixels of the image.\n",
    "        transforms.CenterCrop(224),\n",
    "        # Convert the PIL Image to a PyTorch tensor.\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize the tensor with the provided mean and standard deviation.\n",
    "        transforms.Normalize(mean=mean, std=std),\n",
    "    ]\n",
    "\n",
    "    # Combine the augmentation and main transformations into a single pipeline.\n",
    "    transform = transforms.Compose(augmentations_transforms + main_transforms)\n",
    "    # Return the final composed transform object.\n",
    "    return transform"
   ],
   "id": "8b0631a8e9a508fc",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T14:42:14.944159Z",
     "start_time": "2025-12-01T14:42:14.934138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create the augmentation and preprocessing pipeline, providing the normalization stats.\n",
    "augmentation_transform = get_augmentation_transform(mean=mean, std=std)\n",
    "\n",
    "# Initialize a new dataset instance that will use the augmentation pipeline.\n",
    "dataset_augmented = FlowerDataset(path_dataset, transform=augmentation_transform)"
   ],
   "id": "f54d75b09ea35315",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T14:42:35.509834Z",
     "start_time": "2025-12-01T14:42:35.500031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SubsetWithTransform(Dataset):\n",
    "    \"\"\"\n",
    "    A wrapper for a PyTorch Subset that applies a specific transformation.\n",
    "\n",
    "    This class allows for applying a different set of transformations to a\n",
    "    subset of a dataset, which is useful for creating distinct training,\n",
    "    validation, or test sets with different preprocessing steps from the\n",
    "    same base dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, subset, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the SubsetWithTransform object.\n",
    "\n",
    "        Args:\n",
    "            subset: A PyTorch Subset object containing a portion of a dataset.\n",
    "            transform (callable, optional): An optional transform to be applied\n",
    "                to the samples within this subset.\n",
    "        \"\"\"\n",
    "        # Store the original subset of the dataset.\n",
    "        self.subset = subset\n",
    "        # Store the transformations to be applied.\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the subset.\n",
    "        \"\"\"\n",
    "        # Return the length of the underlying subset.\n",
    "        return len(self.subset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves a sample and applies the transform.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the transformed image and its label.\n",
    "        \"\"\"\n",
    "        # Get the original image and label from the underlying subset.\n",
    "        image, label = self.subset[idx]\n",
    "        # Check if a transform has been provided.\n",
    "        if self.transform:\n",
    "            # Apply the transform to the image.\n",
    "            image = self.transform(image)\n",
    "        # Return the transformed image and its label.\n",
    "        return image, label"
   ],
   "id": "318a7d05b666445a",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T14:42:39.472117Z",
     "start_time": "2025-12-01T14:42:39.464219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply the augmentation pipeline to the training subset.\n",
    "train_dataset = SubsetWithTransform(train_dataset, transform=augmentation_transform)\n",
    "# Apply the basic preprocessing transform to the validation subset.\n",
    "val_dataset = SubsetWithTransform(val_dataset, transform=transform)\n",
    "# Apply the basic preprocessing transform to the test subset.\n",
    "test_dataset = SubsetWithTransform(test_dataset, transform=transform)"
   ],
   "id": "779316e0855b8a77",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T14:42:44.794175Z",
     "start_time": "2025-12-01T14:42:44.788653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(train_dataset.transform)\n",
    "print(val_dataset.transform)\n",
    "print(test_dataset.transform)"
   ],
   "id": "92bd9ad6d48521d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)\n",
      "    ColorJitter(brightness=(0.8, 1.2), contrast=None, saturation=None, hue=None)\n",
      "    Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "Compose(\n",
      "    Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "Compose(\n",
      "    Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T14:43:01.643400Z",
     "start_time": "2025-12-01T14:43:01.628596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RobustFlowerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class with robust error handling for loading images.\n",
    "\n",
    "    This class is designed to gracefully handle issues with individual data\n",
    "    samples, such as corrupted files or incorrect formats. It logs any errors\n",
    "    and attempts to load a different sample instead of crashing.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the dataset object.\n",
    "\n",
    "        Args:\n",
    "            root_dir (str): The root directory where the dataset is stored.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        # Store the root directory path.\n",
    "        self.root_dir = root_dir\n",
    "        # Construct the full path to the image directory.\n",
    "        self.img_dir = os.path.join(root_dir, \"jpg\")\n",
    "        # Store the optional transformations.\n",
    "        self.transform = transform\n",
    "        # Load and process the labels from the corresponding file.\n",
    "        self.labels = self.load_and_correct_labels()\n",
    "        # Initialize a list to keep track of any errors encountered.\n",
    "        self.error_logs = []\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves a sample, handling errors by trying the next available item.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the image and its label.\n",
    "        \"\"\"\n",
    "        # Loop to attempt loading a valid sample, preventing an infinite loop.\n",
    "        for attempt in range(len(self)):\n",
    "            # Attempt to load and process the sample.\n",
    "            try:\n",
    "                # Retrieve the image using the helper method.\n",
    "                image = self.retrieve_image(idx)\n",
    "                # Check if a transform has been provided.\n",
    "                if self.transform:\n",
    "                    # Apply the transform to the image.\n",
    "                    image = self.transform(image)\n",
    "                # Get the label for the current index.\n",
    "                label = self.labels[idx]\n",
    "                # Return the valid image and its corresponding label.\n",
    "                return image, label\n",
    "            # Catch any exception that occurs during the process.\n",
    "            except Exception as e:\n",
    "                # Log the error with its index and message.\n",
    "                self.log_error(idx, e)\n",
    "                # Move to the next index, wrapping around if necessary.\n",
    "                idx = (idx + 1) % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        # The total number of samples is the number of labels.\n",
    "        return len(self.labels)\n",
    "\n",
    "    def retrieve_image(self, idx):\n",
    "        \"\"\"\n",
    "        Loads and validates a single image from disk.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the image to load.\n",
    "\n",
    "        Returns:\n",
    "            PIL.Image.Image: The validated and loaded image object.\n",
    "        \"\"\"\n",
    "        # Construct the image filename based on the index.\n",
    "        img_name = f\"image_{idx+1:05d}.jpg\"\n",
    "        # Construct the full path to the image file.\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        # Open the image file to check its integrity without loading fully.\n",
    "        with Image.open(img_path) as img:\n",
    "            # Perform a quick verification of the file's structure.\n",
    "            img.verify()\n",
    "        # Re-open the image file after successful verification.\n",
    "        image = Image.open(img_path)\n",
    "        # Fully load the image data into memory.\n",
    "        image.load()\n",
    "        # Check if the image dimensions are below a minimum threshold.\n",
    "        if image.size[0] < 32 or image.size[1] < 32:\n",
    "            # Raise an error for images that are too small.\n",
    "            raise ValueError(f\"Image too small: {image.size}\")\n",
    "        # Check if the image is not in the RGB color mode.\n",
    "        if image.mode != \"RGB\":\n",
    "            # Convert the image to RGB.\n",
    "            image = image.convert(\"RGB\")\n",
    "        # Return the fully loaded and validated image.\n",
    "        return image\n",
    "\n",
    "    def load_and_correct_labels(self):\n",
    "        \"\"\"\n",
    "        Loads labels from a .mat file and adjusts them.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: An array of zero-indexed integer labels.\n",
    "        \"\"\"\n",
    "        # Load the MATLAB file containing the labels.\n",
    "        self.labels_mat = scipy.io.loadmat(\n",
    "            os.path.join(self.root_dir, \"imagelabels.mat\")\n",
    "        )\n",
    "        # Extract the labels array and correct for zero-based indexing.\n",
    "        labels = self.labels_mat[\"labels\"][0] - 1\n",
    "        # Truncate the dataset to the first 10 labels for quick testing.\n",
    "        labels = labels[:10]\n",
    "        # Return the processed labels.\n",
    "        return labels\n",
    "\n",
    "    def log_error(self, idx, e):\n",
    "        \"\"\"\n",
    "        Records the details of an error encountered during data loading.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the problematic sample.\n",
    "            e (Exception): The exception object that was raised.\n",
    "        \"\"\"\n",
    "        # Construct the filename of the problematic image.\n",
    "        img_name = f\"image_{idx + 1:05d}.jpg\"\n",
    "        # Construct the full path to the image file.\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        # Append a dictionary with error details to the log.\n",
    "        self.error_logs.append(\n",
    "            {\n",
    "                \"index\": idx,\n",
    "                \"error\": str(e),\n",
    "                \"path\": img_path if \"img_path\" in locals() else \"unknown\",\n",
    "            }\n",
    "        )\n",
    "        # Print a warning to the console about the skipped image.\n",
    "        print(f\"Warning: Skipping corrupted image {idx}: {e}\")\n",
    "\n",
    "    def get_error_summary(self):\n",
    "        \"\"\"\n",
    "        Prints a summary of all errors encountered during dataset processing.\n",
    "        \"\"\"\n",
    "        # Check if the error log is empty.\n",
    "        if not self.error_logs:\n",
    "            # Print a message indicating the dataset is clean.\n",
    "            print(\"No errors encountered - dataset is clean!\")\n",
    "        else:\n",
    "            # Print the total number of problematic images found.\n",
    "            print(f\"\\nEncountered {len(self.error_logs)} problematic images:\")\n",
    "            # Iterate through the first few logged errors.\n",
    "            for error in self.error_logs[:5]:\n",
    "                # Print the details of an individual error.\n",
    "                print(f\"  Index {error['index']}: {error['error']}\")\n",
    "            # Check if there are more errors than were displayed.\n",
    "            if len(self.error_logs) > 5:\n",
    "                # Print a summary of the remaining errors.\n",
    "                print(f\"  ... and {len(self.error_logs) - 5} more\")"
   ],
   "id": "6e56d0c01317a8e3",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T14:43:53.688377Z",
     "start_time": "2025-12-01T14:43:53.681910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the path to the directory containing the corrupted dataset.\n",
    "corrupted_dataset_path = './flower_data'\n",
    "\n",
    "# Initialize the robust dataset handler with the path to the corrupted data.\n",
    "robust_dataset = RobustFlowerDataset(corrupted_dataset_path)"
   ],
   "id": "708f2042901af9df",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T14:44:00.546834Z",
     "start_time": "2025-12-01T14:44:00.503069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MonitoredDataset(RobustFlowerDataset):\n",
    "    \"\"\"\n",
    "    Extends a robust dataset class to add performance monitoring.\n",
    "\n",
    "    This class tracks metrics such as how frequently each image is accessed,\n",
    "    how long each access takes, and which images are never loaded. It provides\n",
    "    a summary of these statistics to help diagnose data pipeline issues.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the monitored dataset object.\n",
    "\n",
    "        Args:\n",
    "            *args: Variable length argument list passed to the parent class.\n",
    "            **kwargs: Arbitrary keyword arguments passed to the parent class.\n",
    "        \"\"\"\n",
    "        # Initialize the parent class with all provided arguments.\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Initialize a dictionary to count how many times each index is accessed.\n",
    "        self.access_counts = {}\n",
    "        # Initialize a list to store the load time for each access.\n",
    "        self.load_times = []\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves a sample while monitoring access counts and load times.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: The data sample (e.g., image and label) from the parent class.\n",
    "        \"\"\"\n",
    "        # Import the time module for timing operations.\n",
    "        import time\n",
    "        # Record the start time of the operation.\n",
    "        start_time = time.time()\n",
    "        # Increment the access count for the given index.\n",
    "        self.access_counts[idx] = self.access_counts.get(idx, 0) + 1\n",
    "        # Call the parent class's method to load the data.\n",
    "        result = super().__getitem__(idx)\n",
    "        # Calculate the total time taken to load the sample.\n",
    "        load_time = time.time() - start_time\n",
    "        # Append the calculated load time to the list.\n",
    "        self.load_times.append(load_time)\n",
    "        # Check if the load time exceeds a certain threshold.\n",
    "        if load_time > 1.0:\n",
    "            # Print a warning if a slow load time is detected.\n",
    "            print(f\" Slow load: Image {idx} took {load_time:.2f}s\")\n",
    "        # Return the loaded sample from the parent class.\n",
    "        return result\n",
    "\n",
    "    def print_stats(self):\n",
    "        \"\"\"\n",
    "        Prints a summary of the dataset's access statistics and performance.\n",
    "        \"\"\"\n",
    "        # Print a header for the statistics report.\n",
    "        print(\"\\n=== Pipeline Statistics ===\")\n",
    "        # Display the total number of images in the dataset.\n",
    "        print(f\"Total images: {len(self)}\")\n",
    "        # Display the number of unique images that were accessed.\n",
    "        print(f\"Unique images accessed: {len(self.access_counts)}\")\n",
    "        # Display the total number of errors logged by the parent class.\n",
    "        print(f\"Errors encountered: {len(self.error_logs)}\")\n",
    "        # Check if any load times have been recorded.\n",
    "        if self.load_times:\n",
    "            # Calculate the average load time.\n",
    "            avg_time = sum(self.load_times) / len(self.load_times)\n",
    "            # Find the maximum (slowest) load time.\n",
    "            max_time = max(self.load_times)\n",
    "            # Print the average load time in milliseconds.\n",
    "            print(f\"Average load time: {avg_time*1000:.1f} ms\")\n",
    "            # Print the slowest load time in milliseconds.\n",
    "            print(f\"Slowest load: {max_time*1000:.1f} ms\")\n",
    "        # Create a set of all possible indices in the dataset.\n",
    "        all_indices = set(range(len(self)))\n",
    "        # Create a set of all indices that were actually accessed.\n",
    "        accessed_indices = set(self.access_counts.keys())\n",
    "        # Find the set of indices that were never accessed.\n",
    "        never_accessed = all_indices - accessed_indices\n",
    "        # Check if there are any images that were never loaded.\n",
    "        if never_accessed:\n",
    "            # Print a warning message with the count of never-accessed images.\n",
    "            print(f\"\\n WARNING: {len(never_accessed)} images were never loaded!\")\n",
    "            # Show a few examples of the indices that were never accessed.\n",
    "            print(f\"   Examples: {list(never_accessed)[:5]}\")"
   ],
   "id": "72f2ddfb3955a11c",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T14:44:06.995829Z",
     "start_time": "2025-12-01T14:44:06.899941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the monitored dataset with the path to the potentially corrupted data.\n",
    "monitored_dataset = MonitoredDataset(corrupted_dataset_path)\n",
    "\n",
    "# Loop through every index in the dataset to simulate a full pass.\n",
    "# Iterate through the dataset to trigger monitoring\n",
    "for idx in range(len(monitored_dataset)):\n",
    "    # Access the sample at the current index to trigger the monitoring and error-handling logic.\n",
    "    img, label = monitored_dataset[idx]"
   ],
   "id": "26d8cd05523eb960",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T14:44:10.996743Z",
     "start_time": "2025-12-01T14:44:10.989412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print the statistics\n",
    "monitored_dataset.print_stats()"
   ],
   "id": "16f744bf33685bf8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Pipeline Statistics ===\n",
      "Total images: 10\n",
      "Unique images accessed: 10\n",
      "Errors encountered: 0\n",
      "Average load time: 8.9 ms\n",
      "Slowest load: 34.7 ms\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b79ec9df329e7700"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
