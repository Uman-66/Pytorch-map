{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-05T15:48:35.346654Z",
     "start_time": "2025-12-05T15:47:20.511732Z"
    }
   },
   "source": [
    "import helper_utils3\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import helper_utils\n",
    "import torch.nn.functional as F\n",
    "from pprint import pprint\n",
    "\n",
    "helper_utils3.set_seed(15)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T15:48:35.747989Z",
     "start_time": "2025-12-05T15:48:35.725843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "id": "dfb1c4de8fcbeae0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d7f0673e03429308"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T15:55:38.177927Z",
     "start_time": "2025-12-05T15:55:38.158573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FlexibleCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A flexible Convolutional Neural Network with a dynamically created classifier.\n",
    "\n",
    "    This CNN's architecture is defined by the provided hyperparameters,\n",
    "    allowing for a variable number of convolutional layers. The classifier\n",
    "    (fully connected layers) is constructed during the first forward pass\n",
    "    to adapt to the output size of the convolutional feature extractor.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_layers, n_filters, kernel_sizes, dropout_rate, fc_size):\n",
    "        \"\"\"\n",
    "        Initializes the feature extraction part of the CNN.\n",
    "\n",
    "        Args:\n",
    "            n_layers: The number of convolutional blocks to create.\n",
    "            n_filters: A list of integers specifying the number of output\n",
    "                       filters for each convolutional block.\n",
    "            kernel_sizes: A list of integers specifying the kernel size for\n",
    "                          each convolutional layer.\n",
    "            dropout_rate: The dropout probability to be used in the classifier.\n",
    "            fc_size: The number of neurons in the hidden fully connected layer.\n",
    "        \"\"\"\n",
    "        super(FlexibleCNN, self).__init__()\n",
    "\n",
    "        # Initialize an empty list to hold the convolutional blocks\n",
    "        blocks = []\n",
    "        # Set the initial number of input channels for RGB images\n",
    "        in_channels = 3\n",
    "\n",
    "        # Loop to construct each convolutional block\n",
    "        for i in range(n_layers):\n",
    "\n",
    "            # Get the parameters for the current convolutional layer\n",
    "            out_channels = n_filters[i]\n",
    "            kernel_size = kernel_sizes[i]\n",
    "            # Calculate padding to maintain the input spatial dimensions ('same' padding)\n",
    "            padding = (kernel_size - 1) // 2\n",
    "\n",
    "            # Define a block as a sequence of Conv, ReLU, and MaxPool layers\n",
    "            block = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "\n",
    "            # Add the newly created block to the list\n",
    "            blocks.append(block)\n",
    "\n",
    "            # Update the number of input channels for the next block\n",
    "            in_channels = out_channels\n",
    "\n",
    "        # Combine all blocks into a single feature extractor module\n",
    "        self.features = nn.Sequential(*blocks)\n",
    "\n",
    "        # Store hyperparameters needed for building the classifier later\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.fc_size = fc_size\n",
    "\n",
    "        # The classifier will be initialized dynamically in the forward pass\n",
    "        self.classifier = None\n",
    "\n",
    "    def _create_classifier(self, flattened_size, device):\n",
    "        \"\"\"\n",
    "        Dynamically creates and initializes the classifier part of the network.\n",
    "\n",
    "        This helper method is called during the first forward pass to build the\n",
    "        fully connected layers based on the feature map size from the\n",
    "        convolutional base.\n",
    "\n",
    "        Args:\n",
    "            flattened_size: The number of input features for the first linear\n",
    "                            layer, determined from the flattened feature map.\n",
    "            device: The device to which the new classifier layers should be moved.\n",
    "        \"\"\"\n",
    "        # Define the classifier's architecture\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(flattened_size, self.fc_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(self.fc_size, 10)  # Assumes 10 output classes (e.g., CIFAR-10)\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x: The input tensor of shape (batch_size, channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            The output logits from the classifier.\n",
    "        \"\"\"\n",
    "        # Get the device of the input tensor to ensure consistency\n",
    "        device = x.device\n",
    "\n",
    "        # Pass the input through the feature extraction layers\n",
    "        x = self.features(x)\n",
    "\n",
    "        # Flatten the feature map to prepare it for the fully connected layers\n",
    "        flattened = torch.flatten(x, 1)\n",
    "        flattened_size = flattened.size(1)\n",
    "\n",
    "        # If the classifier has not been created yet, initialize it\n",
    "        if self.classifier is None:\n",
    "            self._create_classifier(flattened_size, device)\n",
    "\n",
    "        # Pass the flattened features through the classifier to get the final output\n",
    "        return self.classifier(flattened)"
   ],
   "id": "f28cd8622e08429d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "33b2c2b3d0d49522"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T18:19:52.675692Z",
     "start_time": "2025-12-05T18:19:52.668635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def objective(trial, device):\n",
    "    \"\"\"\n",
    "    Defines the objective function for hyperparameter optimization using Optuna.\n",
    "\n",
    "    For each trial, this function samples a set of hyperparameters,\n",
    "    constructs a model, trains it for a fixed number of epochs, evaluates\n",
    "    its performance on a validation set, and returns the accuracy. Optuna\n",
    "    uses the returned accuracy to guide its search for the best\n",
    "    hyperparameter combination.\n",
    "\n",
    "    Args:\n",
    "        trial: An Optuna `Trial` object, used to sample hyperparameters.\n",
    "        device: The device ('cpu' or 'cuda') for model training and evaluation.\n",
    "\n",
    "    Returns:\n",
    "        The validation accuracy of the trained model as a float.\n",
    "    \"\"\"\n",
    "    # Sample hyperparameters for the feature extractor using the Optuna trial\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    n_filters = [trial.suggest_int(f\"n_filters_{i}\", 16, 128) for i in range(n_layers)]\n",
    "    kernel_sizes = [trial.suggest_categorical(f\"kernel_size_{i}\", [3, 5]) for i in range(n_layers)]\n",
    "\n",
    "    # Sample hyperparameters for the classifier\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "    fc_size = trial.suggest_int(\"fc_size\", 64, 256)\n",
    "\n",
    "    # Instantiate the model with the sampled hyperparameters\n",
    "    model = FlexibleCNN(n_layers, n_filters, kernel_sizes, dropout_rate, fc_size).to(device)\n",
    "\n",
    "    # Define fixed training parameters: learning rate, loss function, and optimizer\n",
    "    learning_rate = 0.001\n",
    "    loss_fcn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Define fixed data loading parameters and create data loaders\n",
    "    batch_size = 128\n",
    "    train_loader, val_loader = helper_utils3.get_dataset_dataloaders(batch_size=batch_size)\n",
    "\n",
    "    # Define the fixed number of epochs for training\n",
    "    n_epochs = 10\n",
    "    # Train the model using a helper function\n",
    "    helper_utils3.train_model(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        train_dataloader=train_loader,\n",
    "        n_epochs=n_epochs,\n",
    "        loss_fcn=loss_fcn,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Evaluate the trained model's accuracy on the validation set\n",
    "    accuracy = helper_utils3.evaluate_accuracy(model, val_loader, device)\n",
    "\n",
    "    # Return the final accuracy for this trial\n",
    "    return accuracy"
   ],
   "id": "6c6845212785fd0e",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T18:19:53.027890Z",
     "start_time": "2025-12-05T18:19:53.023461Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b6d742d995a5b4de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-12-05T18:19:53.430677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction='maximize') # The goal in this case is to maximize accuracy\n",
    "\n",
    "# Start the optimization process (it takes about 8 minutes for 20 trials)\n",
    "n_trials = 20\n",
    "study.optimize(lambda trial: objective(trial, device), n_trials=n_trials) # use more trials in practice"
   ],
   "id": "c6d45a8b6d7f4e26",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-05 23:19:53,432] A new study created in memory with name: no-name-7d1c0712-68f9-4f12-8af9-d614d0a248d8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Current Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5a92c9398f994553a74ff8efea330da2"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Current Batch:   0%|          | 0/63 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fef5bf2dbd404976839afda8b0b6b5dd"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train Loss: 1.7406\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Extract the dataframe with the results\n",
    "df = study.trials_dataframe()\n",
    "\n",
    "df"
   ],
   "id": "46ac41bdb8f50149"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Extract and print the best trial\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(f\"  Value (Accuracy): {best_trial.value:.4f}\")\n",
    "\n",
    "print(\"  Hyperparameters:\")\n",
    "pprint(best_trial.params)"
   ],
   "id": "180f4e45cf1ed6df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plotting the optimization history\n",
    "optuna.visualization.matplotlib.plot_optimization_history(study)\n",
    "plt.title('Optimization History')\n",
    "plt.show()\n",
    "\n",
    "# Importance of hyperparameters\n",
    "optuna.visualization.matplotlib.plot_param_importances(study)\n",
    "plt.show()\n",
    "\n",
    "ax = optuna.visualization.matplotlib.plot_parallel_coordinate(\n",
    "    study, params=['n_layers', 'n_filters_0', 'kernel_size_0', 'dropout_rate', 'fc_size']\n",
    ")\n",
    "fig = ax.figure\n",
    "fig.set_size_inches(12, 6, forward=True)  # forward=True updates the canvas\n",
    "fig.tight_layout()"
   ],
   "id": "38b7e4a85bfdf217"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class FlexibleSimpleCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple, flexible Convolutional Neural Network.\n",
    "\n",
    "    This network consists of two convolutional layers, each followed by a\n",
    "    max-pooling layer, and two fully connected layers. The number of filters\n",
    "    in the convolutional layers and the size of the hidden linear layer are\n",
    "    configurable, making the architecture adaptable to different requirements.\n",
    "    \"\"\"\n",
    "    def __init__(self, conv1_out, conv2_out, fc_size, num_classes):\n",
    "        \"\"\"\n",
    "        Initializes the layers of the CNN.\n",
    "\n",
    "        Args:\n",
    "            conv1_out: The number of output channels for the first\n",
    "                       convolutional layer.\n",
    "            conv2_out: The number of output channels for the second\n",
    "                       convolutional layer.\n",
    "            fc_size: The number of neurons in the hidden fully connected layer.\n",
    "            num_classes: The number of output classes for the final layer.\n",
    "        \"\"\"\n",
    "        super(FlexibleSimpleCNN, self).__init__()\n",
    "        # Define the first convolutional layer\n",
    "        self.conv1 = nn.Conv2d(3, conv1_out, kernel_size=3, padding=1)\n",
    "        # Define the second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(conv1_out, conv2_out, kernel_size=3, padding=1)\n",
    "        # Define a max pooling layer to be used after each convolution\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Define the first fully connected (hidden) layer\n",
    "        # Assumes input images are 32x32, resulting in an 8x8 feature map after two pooling layers\n",
    "        self.fc1 = nn.Linear(conv2_out * 8 * 8, fc_size)\n",
    "        # Define the final fully connected (output) layer\n",
    "        self.fc2 = nn.Linear(fc_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            x: The input tensor of shape (batch_size, channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            The output logits from the network.\n",
    "        \"\"\"\n",
    "        # Apply the first convolutional block: convolution, ReLU activation, and pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # Apply the second convolutional block\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Flatten the feature map to prepare for the fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Pass through the first fully connected layer with ReLU activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Pass through the final output layer\n",
    "        x = self.fc2(x)\n",
    "        # Return the resulting logits\n",
    "        return x"
   ],
   "id": "b4143626358b054e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def objective_apples(trial, device):\n",
    "    \"\"\"\n",
    "    Defines the Optuna objective function for a CNN on an apple dataset.\n",
    "\n",
    "    For each trial, this function samples hyperparameters for a CNN\n",
    "    architecture, trains the model on a custom apple dataset, and evaluates\n",
    "    its performance. It logs accuracy, precision, and recall, while\n",
    "    returning the F1-score as the primary metric for Optuna to optimize.\n",
    "\n",
    "    Args:\n",
    "        trial: An Optuna `Trial` object used to sample hyperparameters.\n",
    "        device: The device ('cpu' or 'cuda') for model training and evaluation.\n",
    "\n",
    "    Returns:\n",
    "        The F1-score of the trained model on the validation set.\n",
    "    \"\"\"\n",
    "    # Sample a set of hyperparameters for the model architecture\n",
    "    conv1_out = trial.suggest_int(\"conv1_out\", 8, 64, step=8)\n",
    "    conv2_out = trial.suggest_int(\"conv2_out\", 16, 128, step=16)\n",
    "    fc_size = trial.suggest_int(\"fc_size\", 32, 256, step=32)\n",
    "\n",
    "    # Define fixed parameters for the data loaders\n",
    "    img_size = 32\n",
    "    batch_size = 128\n",
    "\n",
    "    # Create the training and validation data loaders for the apple dataset\n",
    "    train_loader, val_loader = helper_utils.get_apples_dataset_dataloaders(\n",
    "        img_size=img_size,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Specify the number of output classes for the dataset\n",
    "    num_classes = 2\n",
    "    # Create an instance of the model with the sampled hyperparameters\n",
    "    model = FlexibleSimpleCNN(\n",
    "        conv1_out=conv1_out,\n",
    "        conv2_out=conv2_out,\n",
    "        fc_size=fc_size,\n",
    "        num_classes=num_classes\n",
    "    ).to(device)\n",
    "\n",
    "    # Define fixed training components: learning rate, optimizer, and loss function\n",
    "    learning_rate = 0.001\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fcn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Set the fixed number of epochs for the training loop\n",
    "    n_epochs = 5\n",
    "    # Train the model using a helper function\n",
    "    helper_utils.train_model(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        train_dataloader=train_loader,\n",
    "        n_epochs=n_epochs,\n",
    "        loss_fcn=loss_fcn,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Evaluate the trained model on the validation set to get performance metrics\n",
    "    accuracy, precision, recall, f1 = helper_utils.evaluate_metrics(\n",
    "        model, val_loader, device, num_classes=2\n",
    "    )\n",
    "\n",
    "    # Log additional metrics to the Optuna trial for more detailed analysis\n",
    "    trial.set_user_attr(\"accuracy\", accuracy)\n",
    "    trial.set_user_attr(\"precision\", precision)\n",
    "    trial.set_user_attr(\"recall\", recall)\n",
    "\n",
    "    # Return the F1-score as the objective value for Optuna to maximize\n",
    "    return f1"
   ],
   "id": "b4e0439f07bbb02e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "seed = 42\n",
    "helper_utils.set_seed(seed)\n",
    "\n",
    "sampler = optuna.samplers.TPESampler(seed=seed)  # Use TPE sampler (the default sampler in Optuna)\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study_apples = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "\n",
    "n_trials = 10\n",
    "study_apples.optimize(lambda trial: objective_apples(trial, device), n_trials=n_trials)"
   ],
   "id": "bdc7037cf11945e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_apples_study = study_apples.trials_dataframe()\n",
    "\n",
    "df_apples_study"
   ],
   "id": "d6da8b3b208ac779"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Run with Grid Search Sampler\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    \"conv1_out\": list(range(8, 65, 8)),       # [8, 16, 24, 32, 40, 48, 56, 64]\n",
    "    \"conv2_out\": list(range(16, 129, 16)),    # [16, 32, 48, 64, 80, 96, 112, 128]\n",
    "    \"fc_size\":   list(range(32, 257, 32))     # [32, 64, 96, 128, 160, 192, 224, 256]\n",
    "}\n",
    "\n",
    "# Create a GridSampler with the defined grid\n",
    "grid_sampler = optuna.samplers.GridSampler(param_grid, seed=seed)  # Use seed for reproducibility\n",
    "\n",
    "# Create a study object with the GridSampler\n",
    "study_grid = optuna.create_study(direction='maximize', sampler=grid_sampler)\n",
    "\n",
    "study_grid.optimize(lambda trial: objective_apples(trial, device), n_trials=n_trials)# Plotting the optimization history\n",
    "optuna.visualization.matplotlib.plot_optimization_history(study_apples)\n",
    "plt.title('Optimization History')\n",
    "plt.show()\n",
    "\n",
    "# Plotting the optimization history\n",
    "optuna.visualization.matplotlib.plot_optimization_history(study_grid)\n",
    "plt.title('Optimization History')\n",
    "plt.show()"
   ],
   "id": "3792c6125db2d5d2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T04:14:40.187572Z",
     "start_time": "2025-12-07T04:14:25.841184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "from IPython.display import Image as DisplayImage\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torchvision import datasets\n",
    "from torchvision.io import decode_image\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import helper_utils"
   ],
   "id": "2bf6017a22795dce",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check if the OxfordIIITPet data folder exists\n",
    "ox3_pet_data_path = './oxford3pet_data'\n",
    "if os.path.exists(ox3_pet_data_path) and os.path.isdir(ox3_pet_data_path):\n",
    "    ox3_pet_download = False  # Data folder exists, will be loaded from\n",
    "else:\n",
    "    ox3_pet_download = True  # Data folder doesn't exist, will be downloaded"
   ],
   "id": "3d272de3e935b6a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load an image\n",
    "image = Image.open('./images/mangoes.jpg')\n",
    "\n",
    "# Dimensions of the original PIL image\n",
    "print(\"Original PIL Image Dimensions:\", image.size)\n",
    "print(f\"The maximum pixel value is: {image.getextrema()[0][1]}, and the minimum is: {image.getextrema()[0][0]}\")"
   ],
   "id": "866c6d7f6d21cfe7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Convert the PIL image to a PyTorch Tensor\n",
    "img_tensor = transforms.ToTensor()(image)\n",
    "\n",
    "# Dimensions (shape) of the tensor\n",
    "# [C, H, W] format\n",
    "print(f\"Dimensions After Converting to a Tensor: {img_tensor.shape}\")\n",
    "print(f\"The maximum pixel value is: {img_tensor.max()}, and the minimum is: {img_tensor.min()}\")"
   ],
   "id": "9b7ea162b4e8d0b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Convert the tensor back to a PIL image\n",
    "img_pil = transforms.ToPILImage()(img_tensor)\n",
    "\n",
    "# Dimensions of the converted back PIL image\n",
    "print(\"Dimensions After Converting Back to PIL:\", img_pil.size)"
   ],
   "id": "dd2aba5d9fb94785"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Visualize the original and converted images\n",
    "helper_utils.show_images([image, img_pil], titles=(\"Original Image\", \"After PIL→Tensor→PIL conversion\"))"
   ],
   "id": "fa4876a4e2969615"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the path to the image file.\n",
    "image_path = './images/apples.jpg'\n",
    "\n",
    "# Load the image\n",
    "image = decode_image(image_path)\n",
    "\n",
    "print(f\"Image tensor dimensions: {image.shape}\")\n",
    "print(f\"Image tensor dtype: {image.dtype}\")\n",
    "print(f\"The maximum pixel value is: {image.max()}, and the minimum is: {image.min()}\\n\")"
   ],
   "id": "4a9eebde3e1c37f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Use the DisplayImage to render the image\n",
    "DisplayImage(image_path, width=500, height=500)"
   ],
   "id": "3d634f884130e16f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Create a batch of images (./images/ contains only 6 images). The images are loaded as 300x300 pixels\n",
    "images_tensor = helper_utils.load_images(\"./images/\")\n",
    "\n",
    "# The size is 6 images x 3 color channels x 300 pixels height x 300 pixels width\n",
    "print(f\"Image tensor dimensions: {images_tensor.shape}\")"
   ],
   "id": "599f7bdf9f54cf77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Make a grid from the loaded images (2 rows of 3 for 6 images)\n",
    "grid = vutils.make_grid(tensor=images_tensor, nrow=3, padding=5, normalize=True)\n",
    "\n",
    "# the shape comes from\n",
    "# num_images/nrow*pixel_height+(num_images/nrow+1)*padding = 2*300+3*5 = 615\n",
    "# nrow*pixel_width+(nrow+1)*padding = 3*300+4*5 = 920\n",
    "print(f\"Image tensor dimensions: {grid.shape}\")\n",
    "print(f\"The maximum pixel value is: {grid.max()}, and the minimum is: {grid.min()}\\n\")\n",
    "\n",
    "# Display the grid of images using a helper function\n",
    "helper_utils.display_grid(grid)"
   ],
   "id": "9173d3ecf99a66c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the path to save the image file.\n",
    "image_path = \"./fruits_grid.png\"\n",
    "\n",
    "# Save the grid as a PNG image\n",
    "vutils.save_image(tensor=grid, fp=image_path)"
   ],
   "id": "dbc1cc3be383f9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Use the DisplayImage to render the image\n",
    "DisplayImage(image_path)"
   ],
   "id": "d2bef587aeec9fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "original_image = Image.open('./images/strawberries.jpg')# Define the resize transformation (50x50 square)",
   "id": "e72f1ba6f07af257"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the resize transformation (50x50 square)\n",
    "resize_transform = transforms.Resize(size=50)\n",
    "\n",
    "# Apply the transformation\n",
    "resized_image = resize_transform(original_image)"
   ],
   "id": "99f02c274864335"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# (Width, Height)\n",
    "print(f\"Original Dimensions: {original_image.size}\")\n",
    "print(f\"Resized Dimensions:  {resized_image.size}\\n\")\n",
    "\n",
    "helper_utils.show_images(\n",
    "    images=[original_image, resized_image],\n",
    "    titles=(\"Original\", \"Resized to (50, 50)\")\n",
    ")"
   ],
   "id": "a040e6b099ca5610"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the center crop transformation (256x256)\n",
    "center_crop_transform = transforms.CenterCrop(size=256)\n",
    "\n",
    "# Apply the transformation\n",
    "cropped_image = center_crop_transform(original_image)"
   ],
   "id": "34a9807744ff7afc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the center crop transformation (256x256)\n",
    "center_crop_transform = transforms.CenterCrop(size=256)\n",
    "\n",
    "# Apply the transformation\n",
    "cropped_image = center_crop_transform(original_image)"
   ],
   "id": "95ff0e987b000c75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# (Width, Height)\n",
    "print(f\"Original Dimensions: {original_image.size}\")\n",
    "print(f\"Cropped Dimensions:  {cropped_image.size}\\n\")\n",
    "\n",
    "helper_utils.show_images(\n",
    "    images=[original_image, cropped_image],\n",
    "    titles=(\"Original\", \"Center Crop (256, 256)\")\n",
    ")"
   ],
   "id": "366c955be8e82fad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the RandomResizedCrop transformation (224x224)\n",
    "random_resized_crop_transform = transforms.RandomResizedCrop(size=224)\n",
    "\n",
    "# Apply the transformation\n",
    "cropped_resized_image_1 = random_resized_crop_transform(original_image)\n",
    "cropped_resized_image_2 = random_resized_crop_transform(original_image)\n",
    "cropped_resized_image_3 = random_resized_crop_transform(original_image)"
   ],
   "id": "69df99f72c0b2156"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# (Width, Height)\n",
    "print(f\"Original Dimensions: {original_image.size}\")\n",
    "print(f\"RandomResizedCrop 1 Dimensions:  {cropped_resized_image_1.size}\")\n",
    "print(f\"RandomResizedCrop 2 Dimensions:  {cropped_resized_image_2.size}\")\n",
    "print(f\"RandomResizedCrop 3 Dimensions:  {cropped_resized_image_3.size}\\n\")\n",
    "\n",
    "helper_utils.show_images(\n",
    "    images=[original_image, cropped_resized_image_1],\n",
    "    titles=(\"Original (2048, 2048)\", \"RandomResizedCrop 1 (224, 224)\")\n",
    ")\n",
    "helper_utils.show_images(\n",
    "    images=[cropped_resized_image_2, cropped_resized_image_3],\n",
    "    titles=(\"RandomResizedCrop 2 (224, 224)\", \"RandomResizedCrop 3 (224, 224)\")\n",
    ")\n"
   ],
   "id": "1b21e3f18f5a276c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the horizontal flip transformation\n",
    "# Set p=1.0 to guarantee the flip happens for this demonstration\n",
    "flip_transform = transforms.RandomHorizontalFlip(p=1.0)\n",
    "\n",
    "# Apply the transformation\n",
    "flipped_image = flip_transform(original_image)"
   ],
   "id": "3fbb5705ea8242e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the ColorJitter transformation\n",
    "# The values determine the random range for each property.\n",
    "jitter_transform = transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5)\n",
    "\n",
    "# Apply the transformation\n",
    "jittered_image = jitter_transform(original_image)"
   ],
   "id": "d1306f6cfe3022a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "helper_utils.show_images(\n",
    "    images=[original_image, jittered_image],\n",
    "    titles=(\"Original\", \"ColorJitter\")\n",
    ")"
   ],
   "id": "5c956418cb3df2f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T04:22:43.034832Z",
     "start_time": "2025-12-07T04:22:43.022884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SaltAndPepperNoise:\n",
    "    \"\"\"\n",
    "    A custom transform to add salt and pepper noise to a PIL image.\n",
    "\n",
    "    Args:\n",
    "        salt_vs_pepper (float): The ratio of salt to pepper noise.\n",
    "                                (e.g., 0.5 is an equal amount of each).\n",
    "        amount (float): The total proportion of pixels to be affected by noise.\n",
    "    \"\"\"\n",
    "    def __init__(self, salt_vs_pepper=0.5, amount=0.04):\n",
    "        self.s_vs_p = salt_vs_pepper\n",
    "        self.amount = amount\n",
    "\n",
    "    def __call__(self, image):\n",
    "        # Make a copy of the image\n",
    "        output = np.copy(np.array(image))\n",
    "\n",
    "        # Add Salt Noise\n",
    "        num_salt = np.ceil(self.amount * image.size[0] * image.size[1] * self.s_vs_p)\n",
    "        # Generate random coordinates for salt noise\n",
    "        coords = [np.random.randint(0, i - 1, int(num_salt)) for i in image.size]\n",
    "        # Set pixels to white\n",
    "        output[coords[1], coords[0]] = 255\n",
    "\n",
    "        # Add Pepper Noise\n",
    "        num_pepper = np.ceil(self.amount * image.size[0] * image.size[1] * (1.0 - self.s_vs_p))\n",
    "        # Generate random coordinates for pepper noise\n",
    "        coords = [np.random.randint(0, i - 1, int(num_pepper)) for i in image.size]\n",
    "        # Set pixels to black\n",
    "        output[coords[1], coords[0]] = 0\n",
    "\n",
    "        # Convert the NumPy array back to a PIL image\n",
    "        return Image.fromarray(output)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + f'(salt_vs_pepper={self.s_vs_p}, amount={self.amount})'"
   ],
   "id": "786028d28e3b3f58",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T04:23:16.182022Z",
     "start_time": "2025-12-07T04:23:14.818633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Instantiate your custom transformation\n",
    "sp_transform = SaltAndPepperNoise(salt_vs_pepper=0.5, amount=0.5)\n",
    "\n",
    "# Apply the transformation\n",
    "sp_image = sp_transform(original_image)"
   ],
   "id": "c0c90478e4e76497",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'original_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      2\u001B[39m sp_transform = SaltAndPepperNoise(salt_vs_pepper=\u001B[32m0.5\u001B[39m, amount=\u001B[32m0.5\u001B[39m)\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# Apply the transformation\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m sp_image = sp_transform(\u001B[43moriginal_image\u001B[49m)\n",
      "\u001B[31mNameError\u001B[39m: name 'original_image' is not defined"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "helper_utils.show_images(\n",
    "    images=[original_image, sp_image],\n",
    "    titles=(\"Original\", \"With Salt & Pepper Noise\")\n",
    ")"
   ],
   "id": "36587004d40dae35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Convert to tensor (scales to [0, 1])\n",
    "tensor_image = transforms.ToTensor()(original_image)\n",
    "\n",
    "# Define the normalization transform using ImageNet stats\n",
    "normalize_transform = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "# Apply the transformation\n",
    "normalized_tensor = normalize_transform(tensor_image)"
   ],
   "id": "65ddea6ff340fde8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def calculate_mean_std(dataset):\n",
    "    \"\"\"\n",
    "    Calculates the mean and standard deviation of a PyTorch dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (torch.utils.data.Dataset): The dataset for which to\n",
    "                                            calculate the stats. It should\n",
    "                                            return image tensors.\n",
    "\n",
    "    Returns:\n",
    "        (torch.Tensor, torch.Tensor): A tuple containing the mean and\n",
    "                                      standard deviation tensors, each of\n",
    "                                      shape (C,).\n",
    "    \"\"\"\n",
    "    # Create a DataLoader to iterate through the dataset in batches for efficiency.\n",
    "    # shuffle=False because the order of images doesn't matter for this calculation.\n",
    "    loader = data.DataLoader(dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Initialize tensors to store the sum of pixel values for each (RGB) channel.\n",
    "    channel_sum = torch.zeros(3)\n",
    "    # Initialize tensors to store the sum of squared pixel values for each channel.\n",
    "    channel_sum_sq = torch.zeros(3)\n",
    "    # Initialize a counter for the total number of pixels.\n",
    "    num_pixels = 0\n",
    "\n",
    "    # Wrap the loader with tqdm to create a progress bar for monitoring.\n",
    "    for images, _ in tqdm(loader, desc=\"Calculating Dataset Stats\"):\n",
    "        # Add the total number of pixels in this batch to the running total.\n",
    "        num_pixels += images.size(0) * images.size(2) * images.size(3)\n",
    "\n",
    "        # Sum the pixel values across the batch, height, and width dimensions,\n",
    "        # leaving only the channel dimension. Add this to the running total.\n",
    "        channel_sum += images.sum(dim=[0, 2, 3])\n",
    "\n",
    "        # Square each pixel value, then sum them up similarly to the step above.\n",
    "        channel_sum_sq += (images ** 2).sum(dim=[0, 2, 3])\n",
    "\n",
    "    # Calculate the mean for each channel.\n",
    "    mean = channel_sum / num_pixels\n",
    "    # Calculate the standard deviation using the formula: sqrt(E[X^2] - E[X]^2)\n",
    "    std = (channel_sum_sq / num_pixels - mean ** 2) ** 0.5\n",
    "\n",
    "    # Return the calculated mean and standard deviation.\n",
    "    return mean, std"
   ],
   "id": "f392ef637dffa180"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define a simple transformation\n",
    "simple_transform = transforms.Compose([\n",
    "    transforms.Resize((100, 100)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load the OxfordIIITPet dataset, applying the simple transform to each image\n",
    "my_dataset = datasets.OxfordIIITPet(root=ox3_pet_data_path,\n",
    "                                    split='test',                 # Specify using the test set\n",
    "                                    download=ox3_pet_download,    # Download if not already present\n",
    "                                    transform=simple_transform    # Apply the defined transformations\n",
    "                                   )\n",
    "\n",
    "# Compute the mean and standard deviation for the dataset\n",
    "dataset_mean, dataset_std = calculate_mean_std(my_dataset)\n",
    "\n",
    "print(f\"\\nCalculation Complete.\")\n",
    "print(f\"Dataset Mean: {dataset_mean}\")\n",
    "print(f\"Dataset Std:  {dataset_std}\")"
   ],
   "id": "ba8fbc7c9d6a7c9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# A simple transform to get a clean, un-augmented version of the images\n",
    "base_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor()\n",
    "    # Skip normalization to keep the image's pixel values in a display-friendly range.\n",
    "])\n",
    "\n",
    "# The full augmentation pipeline with all random transformations\n",
    "full_augmentation_pipeline = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=224),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.05, contrast=0.05),\n",
    "    SaltAndPepperNoise(amount=0.001),\n",
    "    transforms.ToTensor(),\n",
    "    # Using `mean` and `std` values as calculated on the 100x100 images\n",
    "    transforms.Normalize(mean=dataset_mean,\n",
    "                         std=dataset_std)\n",
    "])"
   ],
   "id": "55c04871e64b9395"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the dataset with ONLY the base transforms\n",
    "original_dataset = datasets.OxfordIIITPet(root=ox3_pet_data_path,\n",
    "                                          split='test',\n",
    "                                          download=ox3_pet_download,\n",
    "                                          transform=base_transform\n",
    "                                         )\n",
    "\n",
    "# Create a DataLoader for the original images\n",
    "original_loader = data.DataLoader(original_dataset, batch_size=9, shuffle=True)"
   ],
   "id": "6668c95e4925a3be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get one fixed batch of original images\n",
    "original_images, _ = next(iter(original_loader))\n",
    "\n",
    "# Create a grid from the batch of images, arranging them with 3 images per row.\n",
    "grid = vutils.make_grid(original_images, nrow=3, padding=2)\n",
    "\n",
    "print(\"Original Un-augmented Batch:\\n\")\n",
    "helper_utils.display_grid(grid)"
   ],
   "id": "57dfdd5de2e0f5b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Use a loop to apply different random augmentations\n",
    "for i in range(3):\n",
    "\n",
    "    augmented_batch = []\n",
    "\n",
    "    # Loop through each original image in the fixed batch\n",
    "    for img_tensor in original_images:\n",
    "\n",
    "        # Convert tensor back to PIL image to apply random transforms\n",
    "        img_pil = transforms.ToPILImage()(img_tensor)\n",
    "\n",
    "        # Apply the random augmentation pipeline\n",
    "        augmented_tensor = full_augmentation_pipeline(img_pil)\n",
    "\n",
    "        # Add the augmented tensor to the list for display\n",
    "        augmented_batch.append(augmented_tensor)\n",
    "\n",
    "    # Stack the list of augmented tensors into a single batch tensor\n",
    "    final_batch = torch.stack(augmented_batch)\n",
    "\n",
    "    # Create a grid from the batch of images, arranging them with 3 images per row\n",
    "    grid = vutils.make_grid(final_batch, nrow=3, padding=2)\n",
    "\n",
    "    print(f\"\\nAugmented Batch - Run #{i + 1}\")\n",
    "    helper_utils.display_grid(grid)"
   ],
   "id": "4beccfbf4857f159"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
