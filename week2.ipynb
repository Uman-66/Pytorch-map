{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-05T15:48:35.346654Z",
     "start_time": "2025-12-05T15:47:20.511732Z"
    }
   },
   "source": [
    "import helper_utils3\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import helper_utils\n",
    "import torch.nn.functional as F\n",
    "from pprint import pprint\n",
    "\n",
    "helper_utils3.set_seed(15)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T15:48:35.747989Z",
     "start_time": "2025-12-05T15:48:35.725843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "id": "dfb1c4de8fcbeae0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d7f0673e03429308"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T15:55:38.177927Z",
     "start_time": "2025-12-05T15:55:38.158573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FlexibleCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A flexible Convolutional Neural Network with a dynamically created classifier.\n",
    "\n",
    "    This CNN's architecture is defined by the provided hyperparameters,\n",
    "    allowing for a variable number of convolutional layers. The classifier\n",
    "    (fully connected layers) is constructed during the first forward pass\n",
    "    to adapt to the output size of the convolutional feature extractor.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_layers, n_filters, kernel_sizes, dropout_rate, fc_size):\n",
    "        \"\"\"\n",
    "        Initializes the feature extraction part of the CNN.\n",
    "\n",
    "        Args:\n",
    "            n_layers: The number of convolutional blocks to create.\n",
    "            n_filters: A list of integers specifying the number of output\n",
    "                       filters for each convolutional block.\n",
    "            kernel_sizes: A list of integers specifying the kernel size for\n",
    "                          each convolutional layer.\n",
    "            dropout_rate: The dropout probability to be used in the classifier.\n",
    "            fc_size: The number of neurons in the hidden fully connected layer.\n",
    "        \"\"\"\n",
    "        super(FlexibleCNN, self).__init__()\n",
    "\n",
    "        # Initialize an empty list to hold the convolutional blocks\n",
    "        blocks = []\n",
    "        # Set the initial number of input channels for RGB images\n",
    "        in_channels = 3\n",
    "\n",
    "        # Loop to construct each convolutional block\n",
    "        for i in range(n_layers):\n",
    "\n",
    "            # Get the parameters for the current convolutional layer\n",
    "            out_channels = n_filters[i]\n",
    "            kernel_size = kernel_sizes[i]\n",
    "            # Calculate padding to maintain the input spatial dimensions ('same' padding)\n",
    "            padding = (kernel_size - 1) // 2\n",
    "\n",
    "            # Define a block as a sequence of Conv, ReLU, and MaxPool layers\n",
    "            block = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "\n",
    "            # Add the newly created block to the list\n",
    "            blocks.append(block)\n",
    "\n",
    "            # Update the number of input channels for the next block\n",
    "            in_channels = out_channels\n",
    "\n",
    "        # Combine all blocks into a single feature extractor module\n",
    "        self.features = nn.Sequential(*blocks)\n",
    "\n",
    "        # Store hyperparameters needed for building the classifier later\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.fc_size = fc_size\n",
    "\n",
    "        # The classifier will be initialized dynamically in the forward pass\n",
    "        self.classifier = None\n",
    "\n",
    "    def _create_classifier(self, flattened_size, device):\n",
    "        \"\"\"\n",
    "        Dynamically creates and initializes the classifier part of the network.\n",
    "\n",
    "        This helper method is called during the first forward pass to build the\n",
    "        fully connected layers based on the feature map size from the\n",
    "        convolutional base.\n",
    "\n",
    "        Args:\n",
    "            flattened_size: The number of input features for the first linear\n",
    "                            layer, determined from the flattened feature map.\n",
    "            device: The device to which the new classifier layers should be moved.\n",
    "        \"\"\"\n",
    "        # Define the classifier's architecture\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(flattened_size, self.fc_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(self.fc_size, 10)  # Assumes 10 output classes (e.g., CIFAR-10)\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x: The input tensor of shape (batch_size, channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            The output logits from the classifier.\n",
    "        \"\"\"\n",
    "        # Get the device of the input tensor to ensure consistency\n",
    "        device = x.device\n",
    "\n",
    "        # Pass the input through the feature extraction layers\n",
    "        x = self.features(x)\n",
    "\n",
    "        # Flatten the feature map to prepare it for the fully connected layers\n",
    "        flattened = torch.flatten(x, 1)\n",
    "        flattened_size = flattened.size(1)\n",
    "\n",
    "        # If the classifier has not been created yet, initialize it\n",
    "        if self.classifier is None:\n",
    "            self._create_classifier(flattened_size, device)\n",
    "\n",
    "        # Pass the flattened features through the classifier to get the final output\n",
    "        return self.classifier(flattened)"
   ],
   "id": "f28cd8622e08429d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "33b2c2b3d0d49522"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T18:19:52.675692Z",
     "start_time": "2025-12-05T18:19:52.668635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def objective(trial, device):\n",
    "    \"\"\"\n",
    "    Defines the objective function for hyperparameter optimization using Optuna.\n",
    "\n",
    "    For each trial, this function samples a set of hyperparameters,\n",
    "    constructs a model, trains it for a fixed number of epochs, evaluates\n",
    "    its performance on a validation set, and returns the accuracy. Optuna\n",
    "    uses the returned accuracy to guide its search for the best\n",
    "    hyperparameter combination.\n",
    "\n",
    "    Args:\n",
    "        trial: An Optuna `Trial` object, used to sample hyperparameters.\n",
    "        device: The device ('cpu' or 'cuda') for model training and evaluation.\n",
    "\n",
    "    Returns:\n",
    "        The validation accuracy of the trained model as a float.\n",
    "    \"\"\"\n",
    "    # Sample hyperparameters for the feature extractor using the Optuna trial\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    n_filters = [trial.suggest_int(f\"n_filters_{i}\", 16, 128) for i in range(n_layers)]\n",
    "    kernel_sizes = [trial.suggest_categorical(f\"kernel_size_{i}\", [3, 5]) for i in range(n_layers)]\n",
    "\n",
    "    # Sample hyperparameters for the classifier\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "    fc_size = trial.suggest_int(\"fc_size\", 64, 256)\n",
    "\n",
    "    # Instantiate the model with the sampled hyperparameters\n",
    "    model = FlexibleCNN(n_layers, n_filters, kernel_sizes, dropout_rate, fc_size).to(device)\n",
    "\n",
    "    # Define fixed training parameters: learning rate, loss function, and optimizer\n",
    "    learning_rate = 0.001\n",
    "    loss_fcn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Define fixed data loading parameters and create data loaders\n",
    "    batch_size = 128\n",
    "    train_loader, val_loader = helper_utils3.get_dataset_dataloaders(batch_size=batch_size)\n",
    "\n",
    "    # Define the fixed number of epochs for training\n",
    "    n_epochs = 10\n",
    "    # Train the model using a helper function\n",
    "    helper_utils3.train_model(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        train_dataloader=train_loader,\n",
    "        n_epochs=n_epochs,\n",
    "        loss_fcn=loss_fcn,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Evaluate the trained model's accuracy on the validation set\n",
    "    accuracy = helper_utils3.evaluate_accuracy(model, val_loader, device)\n",
    "\n",
    "    # Return the final accuracy for this trial\n",
    "    return accuracy"
   ],
   "id": "6c6845212785fd0e",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T18:19:53.027890Z",
     "start_time": "2025-12-05T18:19:53.023461Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b6d742d995a5b4de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-12-05T18:19:53.430677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction='maximize') # The goal in this case is to maximize accuracy\n",
    "\n",
    "# Start the optimization process (it takes about 8 minutes for 20 trials)\n",
    "n_trials = 20\n",
    "study.optimize(lambda trial: objective(trial, device), n_trials=n_trials) # use more trials in practice"
   ],
   "id": "c6d45a8b6d7f4e26",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-05 23:19:53,432] A new study created in memory with name: no-name-7d1c0712-68f9-4f12-8af9-d614d0a248d8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Current Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5a92c9398f994553a74ff8efea330da2"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Current Batch:   0%|          | 0/63 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fef5bf2dbd404976839afda8b0b6b5dd"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train Loss: 1.7406\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Extract the dataframe with the results\n",
    "df = study.trials_dataframe()\n",
    "\n",
    "df"
   ],
   "id": "46ac41bdb8f50149"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Extract and print the best trial\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(f\"  Value (Accuracy): {best_trial.value:.4f}\")\n",
    "\n",
    "print(\"  Hyperparameters:\")\n",
    "pprint(best_trial.params)"
   ],
   "id": "180f4e45cf1ed6df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plotting the optimization history\n",
    "optuna.visualization.matplotlib.plot_optimization_history(study)\n",
    "plt.title('Optimization History')\n",
    "plt.show()\n",
    "\n",
    "# Importance of hyperparameters\n",
    "optuna.visualization.matplotlib.plot_param_importances(study)\n",
    "plt.show()\n",
    "\n",
    "ax = optuna.visualization.matplotlib.plot_parallel_coordinate(\n",
    "    study, params=['n_layers', 'n_filters_0', 'kernel_size_0', 'dropout_rate', 'fc_size']\n",
    ")\n",
    "fig = ax.figure\n",
    "fig.set_size_inches(12, 6, forward=True)  # forward=True updates the canvas\n",
    "fig.tight_layout()"
   ],
   "id": "38b7e4a85bfdf217"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class FlexibleSimpleCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple, flexible Convolutional Neural Network.\n",
    "\n",
    "    This network consists of two convolutional layers, each followed by a\n",
    "    max-pooling layer, and two fully connected layers. The number of filters\n",
    "    in the convolutional layers and the size of the hidden linear layer are\n",
    "    configurable, making the architecture adaptable to different requirements.\n",
    "    \"\"\"\n",
    "    def __init__(self, conv1_out, conv2_out, fc_size, num_classes):\n",
    "        \"\"\"\n",
    "        Initializes the layers of the CNN.\n",
    "\n",
    "        Args:\n",
    "            conv1_out: The number of output channels for the first\n",
    "                       convolutional layer.\n",
    "            conv2_out: The number of output channels for the second\n",
    "                       convolutional layer.\n",
    "            fc_size: The number of neurons in the hidden fully connected layer.\n",
    "            num_classes: The number of output classes for the final layer.\n",
    "        \"\"\"\n",
    "        super(FlexibleSimpleCNN, self).__init__()\n",
    "        # Define the first convolutional layer\n",
    "        self.conv1 = nn.Conv2d(3, conv1_out, kernel_size=3, padding=1)\n",
    "        # Define the second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(conv1_out, conv2_out, kernel_size=3, padding=1)\n",
    "        # Define a max pooling layer to be used after each convolution\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Define the first fully connected (hidden) layer\n",
    "        # Assumes input images are 32x32, resulting in an 8x8 feature map after two pooling layers\n",
    "        self.fc1 = nn.Linear(conv2_out * 8 * 8, fc_size)\n",
    "        # Define the final fully connected (output) layer\n",
    "        self.fc2 = nn.Linear(fc_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            x: The input tensor of shape (batch_size, channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            The output logits from the network.\n",
    "        \"\"\"\n",
    "        # Apply the first convolutional block: convolution, ReLU activation, and pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # Apply the second convolutional block\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Flatten the feature map to prepare for the fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Pass through the first fully connected layer with ReLU activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Pass through the final output layer\n",
    "        x = self.fc2(x)\n",
    "        # Return the resulting logits\n",
    "        return x"
   ],
   "id": "b4143626358b054e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def objective_apples(trial, device):\n",
    "    \"\"\"\n",
    "    Defines the Optuna objective function for a CNN on an apple dataset.\n",
    "\n",
    "    For each trial, this function samples hyperparameters for a CNN\n",
    "    architecture, trains the model on a custom apple dataset, and evaluates\n",
    "    its performance. It logs accuracy, precision, and recall, while\n",
    "    returning the F1-score as the primary metric for Optuna to optimize.\n",
    "\n",
    "    Args:\n",
    "        trial: An Optuna `Trial` object used to sample hyperparameters.\n",
    "        device: The device ('cpu' or 'cuda') for model training and evaluation.\n",
    "\n",
    "    Returns:\n",
    "        The F1-score of the trained model on the validation set.\n",
    "    \"\"\"\n",
    "    # Sample a set of hyperparameters for the model architecture\n",
    "    conv1_out = trial.suggest_int(\"conv1_out\", 8, 64, step=8)\n",
    "    conv2_out = trial.suggest_int(\"conv2_out\", 16, 128, step=16)\n",
    "    fc_size = trial.suggest_int(\"fc_size\", 32, 256, step=32)\n",
    "\n",
    "    # Define fixed parameters for the data loaders\n",
    "    img_size = 32\n",
    "    batch_size = 128\n",
    "\n",
    "    # Create the training and validation data loaders for the apple dataset\n",
    "    train_loader, val_loader = helper_utils.get_apples_dataset_dataloaders(\n",
    "        img_size=img_size,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Specify the number of output classes for the dataset\n",
    "    num_classes = 2\n",
    "    # Create an instance of the model with the sampled hyperparameters\n",
    "    model = FlexibleSimpleCNN(\n",
    "        conv1_out=conv1_out,\n",
    "        conv2_out=conv2_out,\n",
    "        fc_size=fc_size,\n",
    "        num_classes=num_classes\n",
    "    ).to(device)\n",
    "\n",
    "    # Define fixed training components: learning rate, optimizer, and loss function\n",
    "    learning_rate = 0.001\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fcn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Set the fixed number of epochs for the training loop\n",
    "    n_epochs = 5\n",
    "    # Train the model using a helper function\n",
    "    helper_utils.train_model(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        train_dataloader=train_loader,\n",
    "        n_epochs=n_epochs,\n",
    "        loss_fcn=loss_fcn,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Evaluate the trained model on the validation set to get performance metrics\n",
    "    accuracy, precision, recall, f1 = helper_utils.evaluate_metrics(\n",
    "        model, val_loader, device, num_classes=2\n",
    "    )\n",
    "\n",
    "    # Log additional metrics to the Optuna trial for more detailed analysis\n",
    "    trial.set_user_attr(\"accuracy\", accuracy)\n",
    "    trial.set_user_attr(\"precision\", precision)\n",
    "    trial.set_user_attr(\"recall\", recall)\n",
    "\n",
    "    # Return the F1-score as the objective value for Optuna to maximize\n",
    "    return f1"
   ],
   "id": "b4e0439f07bbb02e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "seed = 42\n",
    "helper_utils.set_seed(seed)\n",
    "\n",
    "sampler = optuna.samplers.TPESampler(seed=seed)  # Use TPE sampler (the default sampler in Optuna)\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study_apples = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "\n",
    "n_trials = 10\n",
    "study_apples.optimize(lambda trial: objective_apples(trial, device), n_trials=n_trials)"
   ],
   "id": "bdc7037cf11945e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_apples_study = study_apples.trials_dataframe()\n",
    "\n",
    "df_apples_study"
   ],
   "id": "d6da8b3b208ac779"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Run with Grid Search Sampler\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    \"conv1_out\": list(range(8, 65, 8)),       # [8, 16, 24, 32, 40, 48, 56, 64]\n",
    "    \"conv2_out\": list(range(16, 129, 16)),    # [16, 32, 48, 64, 80, 96, 112, 128]\n",
    "    \"fc_size\":   list(range(32, 257, 32))     # [32, 64, 96, 128, 160, 192, 224, 256]\n",
    "}\n",
    "\n",
    "# Create a GridSampler with the defined grid\n",
    "grid_sampler = optuna.samplers.GridSampler(param_grid, seed=seed)  # Use seed for reproducibility\n",
    "\n",
    "# Create a study object with the GridSampler\n",
    "study_grid = optuna.create_study(direction='maximize', sampler=grid_sampler)\n",
    "\n",
    "study_grid.optimize(lambda trial: objective_apples(trial, device), n_trials=n_trials)# Plotting the optimization history\n",
    "optuna.visualization.matplotlib.plot_optimization_history(study_apples)\n",
    "plt.title('Optimization History')\n",
    "plt.show()\n",
    "\n",
    "# Plotting the optimization history\n",
    "optuna.visualization.matplotlib.plot_optimization_history(study_grid)\n",
    "plt.title('Optimization History')\n",
    "plt.show()"
   ],
   "id": "3792c6125db2d5d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2bf6017a22795dce"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
